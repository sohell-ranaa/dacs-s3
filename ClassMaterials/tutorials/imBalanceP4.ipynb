{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **tomato juice dataset**\n",
    "<br>` 'quality' is the target feature for classification `\n",
    "<br>` the other features are chemical properties of our product `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_S34U5S-i69d"
   },
   "source": [
    "**Import the main libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "_import the local library_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add parent folder path where lib folder is\n",
    "import sys\n",
    "if \"..\" not in sys.path:import sys; sys.path.insert(0, '..') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mylib import show_labels_dist, show_metrics, bias_var_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9IZetEZ8jQJm"
   },
   "source": [
    "**Import the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions: 1599 rows, 12 columns\n"
     ]
    }
   ],
   "source": [
    "## file path: windows style\n",
    "df = pd.read_csv('tomatjus.csv')\n",
    "\n",
    "## file path: unix style\n",
    "#df = pd.read_csv('../datasets/tomatjus.csv')\n",
    "\n",
    "# shape method gives the dimensions of the dataset\n",
    "print('Dataset dimensions: {} rows, {} columns'.format(\n",
    "    df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         1599 non-null   float64\n",
      " 1   volatile acidity      1599 non-null   float64\n",
      " 2   citric acid           1599 non-null   float64\n",
      " 3   residual sugar        1599 non-null   float64\n",
      " 4   chlorides             1599 non-null   float64\n",
      " 5   free sulfur dioxide   1599 non-null   float64\n",
      " 6   total sulfur dioxide  1599 non-null   float64\n",
      " 7   density               1599 non-null   float64\n",
      " 8   pH                    1599 non-null   float64\n",
      " 9   sulphates             1599 non-null   float64\n",
      " 10  pulp                  1599 non-null   float64\n",
      " 11  quality               1599 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "***\n",
    "**Data Preparation and EDA** (unique to this dataset)\n",
    "* _Check for missing values_\n",
    "* _Quick visual check of unique values_\n",
    "* _Split the classification feature out of the dataset_\n",
    "* _Check column names of categorical attributes ( for get_dummies() )_\n",
    "* _Check column names of numeric attributes ( for Scaling )_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Let's skip the checking_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**<br>Classification target feature**\n",
    "<br>\"the Right Answers\", or more formally \"the desired outcome\"\n",
    "<br>Must be in a separate dataset for classification ,,,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Make it a multi-class problem, using text labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  divide into classes by giving a range for quality\n",
    "##  Make it a multi-class problem: {3,4,5} {6} {7.8}\n",
    "bins = (2, 5, 6, 8)\n",
    "group_names = ['Average', 'Premium', 'Special']\n",
    "df['quality'] = pd.cut(df['quality'], bins = bins, labels = group_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split the classification feature out of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature being predicted (\"the Right Answer\")\n",
    "labels_col = 'quality'\n",
    "y = df[labels_col]\n",
    "\n",
    "## Features used for prediction \n",
    "# pandas has a lot of rules about returning a 'view' vs. a copy from slice\n",
    "# so we force it to create a new dataframe \n",
    "X = df.copy()\n",
    "X.drop(labels_col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "***\n",
    "**<br>Create Test // Train Datasets**\n",
    "> Split X and y datasets into Train and Test subsets,<br>keeping relative proportions of each class (stratify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=50, \n",
    "                                                    stratify=y)\n",
    "# train_test_split does random selection, \n",
    "#      so we should reset the dataframe indexes\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "y_test.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**<br>Scaling** comes _after_ test // train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'pulp']\n"
     ]
    }
   ],
   "source": [
    "numeri = X.select_dtypes(include=['float64','int64']).columns\n",
    "print(numeri.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the Numeric columns \n",
    "# StandardScaler range: -1 to 1, MinMaxScaler range: zero to 1\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# sklearn docs say \n",
    "#   \"Don't cheat - fit only on training data, then transform both\"\n",
    "#   fit() expects 2D array: reshape(-1, 1) for single col or (1, -1) single row\n",
    "\n",
    "for i in numeri:\n",
    "    arr = np.array(X_train[i])\n",
    "    scale = MinMaxScaler().fit(arr.reshape(-1, 1))\n",
    "    X_train[i] = scale.transform(arr.reshape(len(arr),1))\n",
    "\n",
    "    arr = np.array(X_test[i])\n",
    "    X_test[i] = scale.transform(arr.reshape(len(arr),1))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**<br>Classifier Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('RandomForest', RandomForestClassifier())]\n"
     ]
    }
   ],
   "source": [
    "# prepare list\n",
    "models = []\n",
    "\n",
    "##  --  Linear  --  ## \n",
    "#from sklearn.linear_model import LogisticRegression \n",
    "#models.append ((\"LogReg\",LogisticRegression())) \n",
    "#from sklearn.linear_model import SGDClassifier \n",
    "#models.append ((\"StocGradDes\",SGDClassifier())) \n",
    "#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "#models.append((\"LinearDA\", LinearDiscriminantAnalysis())) \n",
    "#from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis \n",
    "#models.append((\"QuadraticDA\", QuadraticDiscriminantAnalysis())) \n",
    "\n",
    "##  --  Support Vector  --  ## \n",
    "#from sklearn.svm import SVC \n",
    "#models.append((\"SupportVectorClf\", SVC())) \n",
    "#from sklearn.svm import LinearSVC \n",
    "#models.append((\"LinearSVC\", LinearSVC())) \n",
    "#from sklearn.linear_model import RidgeClassifier\n",
    "#models.append ((\"RidgeClf\",RidgeClassifier())) \n",
    "\n",
    "##  --  Non-linear  --  ## \n",
    "#from sklearn.tree import DecisionTreeClassifier \n",
    "#models.append ((\"DecisionTree\",DecisionTreeClassifier())) \n",
    "#from sklearn.naive_bayes import GaussianNB \n",
    "#models.append ((\"GaussianNB\",GaussianNB())) \n",
    "#from sklearn.neighbors import KNeighborsClassifier \n",
    "#models.append((\"K-NNeighbors\", KNeighborsClassifier())) \n",
    "\n",
    "##  --  Ensemble: bagging  --  ## \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "models.append((\"RandomForest\", RandomForestClassifier())) \n",
    "##  --  Ensemble: boosting  --  ## \n",
    "#from sklearn.ensemble import AdaBoostClassifier \n",
    "#models.append((\"AdaBoost\", AdaBoostClassifier())) \n",
    "#from sklearn.ensemble import GradientBoostingClassifier \n",
    "#models.append((\"GradientBoost\", GradientBoostingClassifier())) \n",
    "\n",
    "##  --  NeuralNet (simplest)  --  ## \n",
    "#from sklearn.linear_model import Perceptron \n",
    "#models.append ((\"SingleLayerPtron\",Perceptron())) \n",
    "#from sklearn.neural_network import MLPClassifier \n",
    "#models.append((\"MultiLayerPtron\", MLPClassifier()))\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**<br>Target Label Distributions** (standard block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_train: 1279 rows, 11 columns\n",
      "features_test:  320 rows, 11 columns\n",
      "\n",
      "labels_train: 1279 rows, 1 column\n",
      "labels_test:  320 rows, 1 column\n",
      "\n",
      "Frequency and Distribution of labels\n",
      "         quality  %_train  quality  %_test\n",
      "quality                                   \n",
      "Average      595    46.52      149   46.56\n",
      "Premium      510    39.87      128   40.00\n",
      "Special      174    13.60       43   13.44\n"
     ]
    }
   ],
   "source": [
    "# from our local library\n",
    "show_labels_dist(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "**<br>Fit and Predict** (standard block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro average: unweighted mean per label\n",
      "weighted average: support-weighted mean per label\n",
      "MCC: correlation between prediction and ground truth\n",
      "     (+1 perfect, 0 random prediction, -1 inverse)\n",
      "\n",
      "Confusion Matrix: RandomForest\n",
      "Run Time 0.29 seconds\n",
      "\n",
      "               pred:Average  pred:Premium  pred:Special\n",
      "train:Average           121            26             2\n",
      "train:Premium            31            92             5\n",
      "train:Special             2            20            21\n",
      "\n",
      "~~~~\n",
      "     Average :  FPR = 0.193   FNR = 0.188\n",
      "     Premium :  FPR = 0.240   FNR = 0.281\n",
      "     Special :  FPR = 0.025   FNR = 0.512\n",
      "\n",
      "   macro avg :  FPR = 0.153   FNR = 0.327\n",
      "weighted avg :  FPR = 0.134   FNR = 0.269\n",
      "\n",
      "~~~~\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Average      0.786     0.812     0.799       149\n",
      "     Premium      0.667     0.719     0.692       128\n",
      "     Special      0.750     0.488     0.592        43\n",
      "\n",
      "    accuracy                          0.731       320\n",
      "   macro avg      0.734     0.673     0.694       320\n",
      "weighted avg      0.733     0.731     0.728       320\n",
      "\n",
      "~~~~\n",
      "MCC: Overall :  0.548\n",
      "     Average :  0.618\n",
      "     Premium :  0.474\n",
      "     Special :  0.559\n",
      "\n",
      "Parameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "\n",
    "print('macro average: unweighted mean per label')\n",
    "print('weighted average: support-weighted mean per label')\n",
    "print('MCC: correlation between prediction and ground truth')\n",
    "print('     (+1 perfect, 0 random prediction, -1 inverse)\\n')\n",
    "\n",
    "for name, clf in models:\n",
    "    trs = time()\n",
    "    print('Confusion Matrix:', name)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    ygx = clf.predict(X_test)\n",
    "    results.append((name, ygx))\n",
    "    \n",
    "    tre = time() - trs\n",
    "    print (\"Run Time {} seconds\".format(round(tre,2)) + '\\n')\n",
    "    \n",
    "# Easy way to ensure that the confusion matrix rows and columns\n",
    "#   are labeled exactly as the classifier has coded the classes\n",
    "#   [[note the _ at the end of clf.classes_ ]]\n",
    "\n",
    "    show_metrics(y_test, ygx, clf.classes_)   # from our local library\n",
    "    print('\\nParameters: ', clf.get_params(), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**Bias - Variance Decomposition** (standard block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias // Variance Decomposition: RandomForest\n",
      "   Average bias: 0.269\n",
      "   Average variance: 0.116\n",
      "   Average expected loss: 0.302  \"Goodness\": 0.698\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from our local library\n",
    "# reduce (cross-validation) folds for faster results\n",
    "folds = 10\n",
    "for name, clf in models:\n",
    "    print('Bias // Variance Decomposition:', name)\n",
    "    bias_var_metrics(X_train,X_test,y_train,y_test,clf,folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Methods for imbalanced datasets**<br>\n",
    "> * Over / Under sampling\n",
    "> * Assigning class weights\n",
    "\n",
    "_Use only the training data and labels!_ The idea is that making the model is about using _groups_ of observations to learn _patterns_ that can be used to make an accurate prediction of which class every _individual_ observation drawn from the population belongs to.\n",
    "\n",
    "In other words, the training data should represent the characteristics of the _classes_ to make the predictive model, while the test data should represent the _population_ where the actual mix of classes (distribution) is unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "* Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from yellowbrick.target import ClassBalance\n",
    "# The ClassBalance visualizer has a “compare” mode, \n",
    "#   to create a side-by-side bar chart instead of a single bar chart \n",
    "\n",
    "# Instantiate the visualizer\n",
    "visualizer = ClassBalance()\n",
    "visualizer.fit(y_train, y_test)        # Fit the data to the visualizer\n",
    "_ = visualizer.show()                  # Finalize and render the figure\n",
    "# assign visualizer.show() to a null variable to avoid printing some trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our original datasets before we test the modified ones\n",
    "XtrainOriginal = X_train\n",
    "XtestOriginal = X_test\n",
    "ytrainOriginal = y_train\n",
    "ytestOriginal = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "***\n",
    "**<br>Handling Imbalance with Over-sampling // Under-sampling**\n",
    "<br><br>\n",
    "Sampling with replacement allows duplicate values, so we only do this for the training data, to give it a more balanced set of observations to work with. The _pandas dataframe.groupby.sample_ method does not allow sample_amounts to be larger than the group size if replace is False, but if replace is True then replacement will occur even in groups that could have been downsampled.\n",
    "<br><br>\n",
    "Here we create a dictionary that maps each class to number of samples, then _groupby.apply_ with a lambda to create the sample, and conditional logic to determine with or without replacement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Average': 595, 'Premium': 510, 'Special': 174}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels_col = 'quality'\n",
    "yt = pd.DataFrame(y_train)    # series to dataframe\n",
    "ff = yt[[labels_col]].apply(lambda x: x.value_counts())\n",
    "ss = ff[labels_col].to_dict()\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Average': 500, 'Premium': 450, 'Special': 300}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set new values - anything goes!\n",
    "#ss['Premium'] = 450\n",
    "ss['Premium'] = 450\n",
    "#ss['Average'] = ss['Average'] - ss['Special']\n",
    "ss['Average'] = 500\n",
    "#ss['Special'] = round(ss['Special'] * 2.5)\n",
    "ss['Special'] = 300\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the labels back to the features dataframe \n",
    "xy_train = X_train.copy()\n",
    "xy_train[labels_col] = yt[labels_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>pulp</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.061947</td>\n",
       "      <td>0.239726</td>\n",
       "      <td>0.139241</td>\n",
       "      <td>0.089655</td>\n",
       "      <td>0.024263</td>\n",
       "      <td>0.211268</td>\n",
       "      <td>0.293286</td>\n",
       "      <td>0.128488</td>\n",
       "      <td>0.591304</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Special</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.433628</td>\n",
       "      <td>0.452055</td>\n",
       "      <td>0.278481</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.074523</td>\n",
       "      <td>0.070423</td>\n",
       "      <td>0.091873</td>\n",
       "      <td>0.640969</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.139394</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>Premium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.221239</td>\n",
       "      <td>0.184932</td>\n",
       "      <td>0.151899</td>\n",
       "      <td>0.082759</td>\n",
       "      <td>0.053726</td>\n",
       "      <td>0.183099</td>\n",
       "      <td>0.063604</td>\n",
       "      <td>0.179883</td>\n",
       "      <td>0.382609</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>Premium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.168142</td>\n",
       "      <td>0.232877</td>\n",
       "      <td>0.177215</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.138648</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>0.109541</td>\n",
       "      <td>0.532305</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.193939</td>\n",
       "      <td>0.215385</td>\n",
       "      <td>Average</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0       0.061947          0.239726     0.139241        0.089655   0.024263   \n",
       "1       0.433628          0.452055     0.278481        0.068966   0.074523   \n",
       "2       0.221239          0.184932     0.151899        0.082759   0.053726   \n",
       "3       0.168142          0.232877     0.177215        0.103448   0.138648   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide   density        pH  sulphates  \\\n",
       "0             0.211268              0.293286  0.128488  0.591304   0.333333   \n",
       "1             0.070423              0.091873  0.640969  0.347826   0.139394   \n",
       "2             0.183099              0.063604  0.179883  0.382609   0.121212   \n",
       "3             0.112676              0.109541  0.532305  0.695652   0.193939   \n",
       "\n",
       "       pulp  quality  \n",
       "0  0.800000  Special  \n",
       "1  0.338462  Premium  \n",
       "2  0.753846  Premium  \n",
       "3  0.215385  Average  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice the index numbers - random order from test_train_split()\n",
    "xy_train.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-923e4402ef08>:2: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  balanced_df = xy_train.groupby(labels_col,\n"
     ]
    }
   ],
   "source": [
    "# technically, this is a \"one-liner\" ...\n",
    "balanced_df = xy_train.groupby(labels_col, \n",
    "                               as_index=False, group_keys=False, sort=False\n",
    "                        ).apply(lambda g: g.sample(n=ss[g.name],\n",
    "                                                   replace=(len(g) < ss[g.name])\n",
    "                                                  )).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>pulp</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.389381</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.518987</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.119584</td>\n",
       "      <td>0.070423</td>\n",
       "      <td>0.014134</td>\n",
       "      <td>0.438326</td>\n",
       "      <td>0.234783</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>Special</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.327434</td>\n",
       "      <td>0.130137</td>\n",
       "      <td>0.493671</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.076256</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>0.130742</td>\n",
       "      <td>0.320852</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.630769</td>\n",
       "      <td>Special</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.407080</td>\n",
       "      <td>0.130137</td>\n",
       "      <td>0.455696</td>\n",
       "      <td>0.089655</td>\n",
       "      <td>0.077990</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.088339</td>\n",
       "      <td>0.446402</td>\n",
       "      <td>0.408696</td>\n",
       "      <td>0.321212</td>\n",
       "      <td>0.553846</td>\n",
       "      <td>Special</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.221239</td>\n",
       "      <td>0.321918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082759</td>\n",
       "      <td>0.098787</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>0.028269</td>\n",
       "      <td>0.353157</td>\n",
       "      <td>0.486957</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.476923</td>\n",
       "      <td>Special</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0       0.389381          0.178082     0.518987        0.103448   0.119584   \n",
       "1       0.327434          0.130137     0.493671        0.103448   0.076256   \n",
       "2       0.407080          0.130137     0.455696        0.089655   0.077990   \n",
       "3       0.221239          0.321918     0.000000        0.082759   0.098787   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide   density        pH  sulphates  \\\n",
       "0             0.070423              0.014134  0.438326  0.234783   0.151515   \n",
       "1             0.225352              0.130742  0.320852  0.391304   0.266667   \n",
       "2             0.140845              0.088339  0.446402  0.408696   0.321212   \n",
       "3             0.112676              0.028269  0.353157  0.486957   0.133333   \n",
       "\n",
       "       pulp  quality  \n",
       "0  0.538462  Special  \n",
       "1  0.630769  Special  \n",
       "2  0.553846  Special  \n",
       "3  0.476923  Special  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we reset the index at the end, so it is neat\n",
    "balanced_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature being predicted (\"the Right Answer\")\n",
    "ytrain_b = balanced_df[labels_col]\n",
    "\n",
    "## Features used for prediction \n",
    "Xtrain_b = balanced_df.drop(labels_col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitute the datasets\n",
    "X_train = Xtrain_b\n",
    "y_train = ytrain_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**<br>Target Label Distributions** (standard block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "* Class Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**<br>Fit and Predict** (standard block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro average: unweighted mean per label\n",
      "weighted average: support-weighted mean per label\n",
      "MCC: correlation between prediction and ground truth\n",
      "     (+1 perfect, 0 random prediction, -1 inverse)\n",
      "\n",
      "Confusion Matrix: RandomForest\n",
      "Run Time 0.35 seconds\n",
      "\n",
      "               pred:Average  pred:Premium  pred:Special\n",
      "train:Average           116            29             4\n",
      "train:Premium            31            90             7\n",
      "train:Special             2            20            21\n",
      "\n",
      "~~~~\n",
      "     Average :  FPR = 0.193   FNR = 0.221\n",
      "     Premium :  FPR = 0.255   FNR = 0.297\n",
      "     Special :  FPR = 0.040   FNR = 0.512\n",
      "\n",
      "   macro avg :  FPR = 0.163   FNR = 0.343\n",
      "weighted avg :  FPR = 0.145   FNR = 0.291\n",
      "\n",
      "~~~~\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Average      0.779     0.779     0.779       149\n",
      "     Premium      0.647     0.703     0.674       128\n",
      "     Special      0.656     0.488     0.560        43\n",
      "\n",
      "    accuracy                          0.709       320\n",
      "   macro avg      0.694     0.657     0.671       320\n",
      "weighted avg      0.710     0.709     0.707       320\n",
      "\n",
      "~~~~\n",
      "MCC: Overall :  0.513\n",
      "     Average :  0.586\n",
      "     Premium :  0.443\n",
      "     Special :  0.510\n",
      "\n",
      "Parameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "\n",
    "print('macro average: unweighted mean per label')\n",
    "print('weighted average: support-weighted mean per label')\n",
    "print('MCC: correlation between prediction and ground truth')\n",
    "print('     (+1 perfect, 0 random prediction, -1 inverse)\\n')\n",
    "\n",
    "for name, clf in models:\n",
    "    trs = time()\n",
    "    print('Confusion Matrix:', name)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    ygx = clf.predict(X_test)\n",
    "    results.append((name, ygx))\n",
    "    \n",
    "    tre = time() - trs\n",
    "    print (\"Run Time {} seconds\".format(round(tre,2)) + '\\n')\n",
    "    \n",
    "# Easy way to ensure that the confusion matrix rows and columns\n",
    "#   are labeled exactly as the classifier has coded the classes\n",
    "#   [[note the _ at the end of clf.classes_ ]]\n",
    "\n",
    "    show_metrics(y_test, ygx, clf.classes_)   # from our local library\n",
    "    print('\\nParameters: ', clf.get_params(), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**Bias - Variance Decomposition** (standard block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias // Variance Decomposition: RandomForest\n",
      "   Average bias: 0.281\n",
      "   Average variance: 0.135\n",
      "   Average expected loss: 0.323  \"Goodness\": 0.677\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from our local library\n",
    "# reduce (cross-validation) folds for faster results\n",
    "folds = 20\n",
    "for name, clf in models:\n",
    "    print('Bias // Variance Decomposition:', name)\n",
    "    bias_var_metrics(X_train,X_test,y_train,y_test,clf,folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Handling Imbalance with Class Weights**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Balanced weighting is a widely used method for imbalanced classification models. It involves applying specified class weights for the majority and minority classes that are used in the classifier training process to achieve better model results.\n",
    "\n",
    "Unlike over- or under-sampling (a pre-processing step), balanced weighting does not modify the dataset. Instead, each observation is weighted so that wrong predictions for the minority class are given more weight when the loss value is calculated during the model training process. Weights for the loss function can be arbitrary, but a typical choice is weights based on the distribution of labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**SKlearn Classifiers with Class Weights**\n",
    "<br>A limited number of classifiers can take _class_weight='balanced'_ as an argument. This uses the values of y (labels) to automatically adjust weights inversely proportional to class frequencies in the input data (X) as<br>_n_samples / (n_classes * np.bincount(y))_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('RandomForest', RandomForestClassifier(class_weight='balanced'))]\n"
     ]
    }
   ],
   "source": [
    "# prepare list - these support a class_weight argument\n",
    "models = []\n",
    "\n",
    "##  --  Linear  --  ## \n",
    "#from sklearn.linear_model import LogisticRegression \n",
    "#models.append ((\"LogReg\",LogisticRegression(class_weight='balanced'))) \n",
    "#from sklearn.linear_model import SGDClassifier \n",
    "#models.append ((\"StocGradDes\",SGDClassifier(class_weight='balanced'))) \n",
    "\n",
    "##  --  Support Vector  --  ## \n",
    "#from sklearn.svm import SVC \n",
    "#models.append((\"SupportVectorClf\", SVC(class_weight='balanced'))) \n",
    "#from sklearn.svm import LinearSVC \n",
    "#models.append((\"LinearSVC\", LinearSVC(class_weight='balanced'))) \n",
    "#from sklearn.linear_model import RidgeClassifier\n",
    "#models.append ((\"RidgeClf\",RidgeClassifier(class_weight='balanced'))) \n",
    "\n",
    "##  --  Non-linear  --  ## \n",
    "#from sklearn.tree import DecisionTreeClassifier \n",
    "#models.append ((\"DecisionTree\",DecisionTreeClassifier(class_weight='balanced'))) \n",
    "\n",
    "##  --  Ensemble: bagging  --  ## \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "models.append((\"RandomForest\", RandomForestClassifier(class_weight='balanced'))) \n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**_Make sure we are using the right dataset !!_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = XtrainOriginal\n",
    "y_train = ytrainOriginal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**<br>Fit and Predict** (standard block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro average: unweighted mean per label\n",
      "weighted average: support-weighted mean per label\n",
      "MCC: correlation between prediction and ground truth\n",
      "     (+1 perfect, 0 random prediction, -1 inverse)\n",
      "\n",
      "Confusion Matrix: RandomForest\n",
      "Run Time 0.35 seconds\n",
      "\n",
      "               pred:Average  pred:Premium  pred:Special\n",
      "train:Average           121            26             2\n",
      "train:Premium            28            94             6\n",
      "train:Special             2            22            19\n",
      "\n",
      "~~~~\n",
      "     Average :  FPR = 0.175   FNR = 0.188\n",
      "     Premium :  FPR = 0.250   FNR = 0.266\n",
      "     Special :  FPR = 0.029   FNR = 0.558\n",
      "\n",
      "   macro avg :  FPR = 0.151   FNR = 0.337\n",
      "weighted avg :  FPR = 0.134   FNR = 0.269\n",
      "\n",
      "~~~~\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Average      0.801     0.812     0.807       149\n",
      "     Premium      0.662     0.734     0.696       128\n",
      "     Special      0.704     0.442     0.543        43\n",
      "\n",
      "    accuracy                          0.731       320\n",
      "   macro avg      0.722     0.663     0.682       320\n",
      "weighted avg      0.732     0.731     0.727       320\n",
      "\n",
      "~~~~\n",
      "MCC: Overall :  0.548\n",
      "     Average :  0.636\n",
      "     Premium :  0.478\n",
      "     Special :  0.507\n",
      "\n",
      "Parameters:  {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "\n",
    "print('macro average: unweighted mean per label')\n",
    "print('weighted average: support-weighted mean per label')\n",
    "print('MCC: correlation between prediction and ground truth')\n",
    "print('     (+1 perfect, 0 random prediction, -1 inverse)\\n')\n",
    "\n",
    "for name, clf in models:\n",
    "    trs = time()\n",
    "    print('Confusion Matrix:', name)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    ygx = clf.predict(X_test)\n",
    "    results.append((name, ygx))\n",
    "    \n",
    "    tre = time() - trs\n",
    "    print (\"Run Time {} seconds\".format(round(tre,2)) + '\\n')\n",
    "    \n",
    "# Easy way to ensure that the confusion matrix rows and columns\n",
    "#   are labeled exactly as the classifier has coded the classes\n",
    "#   [[note the _ at the end of clf.classes_ ]]\n",
    "\n",
    "    show_metrics(y_test, ygx, clf.classes_)   # from our local library\n",
    "    print('\\nParameters: ', clf.get_params(), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**Bias - Variance Decomposition** (standard block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias // Variance Decomposition: RandomForest\n",
      "   Average bias: 0.256\n",
      "   Average variance: 0.129\n",
      "   Average expected loss: 0.312  \"Goodness\": 0.688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from our local library\n",
    "# reduce (cross-validation) folds for faster results\n",
    "folds = 20\n",
    "for name, clf in models:\n",
    "    print('Bias // Variance Decomposition:', name)\n",
    "    bias_var_metrics(X_train,X_test,y_train,y_test,clf,folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
