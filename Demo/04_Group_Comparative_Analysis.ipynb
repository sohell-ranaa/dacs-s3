{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Assignment: Comparative Analysis\n",
    "## Network Intrusion Detection - Performance Comparison of Different Classifiers\n",
    "\n",
    "**Module:** CT115-3-M Data Analytics in Cyber Security  \n",
    "**Dataset:** NSL-KDD (Boosted Train + Preprocessed Test)  \n",
    "\n",
    "---\n",
    "\n",
    "### Classifiers Compared:\n",
    "1. **Linear:** Linear Discriminant Analysis (LDA)\n",
    "2. **Ensemble (Bagging):** Random Forest\n",
    "3. **Non-Linear:** K-Nearest Neighbors (KNN)\n",
    "4. **Ensemble (Boosting):** Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Import Libraries and Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from individual notebooks\n",
    "results_path = 'results/'\n",
    "\n",
    "results_files = {\n",
    "    'Linear (LDA)': 'linear_lda_results.json',\n",
    "    'Ensemble - RF': 'ensemble_rf_results.json',\n",
    "    'Non-Linear (KNN)': 'nonlinear_knn_results.json',\n",
    "    'Ensemble - GB': 'ensemble_gb_results.json'\n",
    "}\n",
    "\n",
    "all_results = {}\n",
    "for name, filename in results_files.items():\n",
    "    filepath = os.path.join(results_path, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            all_results[name] = json.load(f)\n",
    "        print(f\"Loaded: {name}\")\n",
    "    else:\n",
    "        print(f\"File not found: {filename} - Run individual notebooks first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Overview of Selected Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create algorithm overview table\n",
    "overview_data = []\n",
    "\n",
    "algorithm_descriptions = {\n",
    "    'Linear (LDA)': {\n",
    "        'Full Name': 'Linear Discriminant Analysis',\n",
    "        'Category': 'Linear',\n",
    "        'Characteristics': 'Assumes linear decision boundaries, projects data to maximize class separability',\n",
    "        'Strengths': 'Fast training, interpretable, works well with normally distributed data',\n",
    "        'Weaknesses': 'Assumes linear separability, sensitive to outliers'\n",
    "    },\n",
    "    'Ensemble - RF': {\n",
    "        'Full Name': 'Random Forest',\n",
    "        'Category': 'Ensemble (Bagging)',\n",
    "        'Characteristics': 'Builds multiple decision trees and aggregates predictions via voting',\n",
    "        'Strengths': 'Robust to overfitting, handles high dimensionality, feature importance',\n",
    "        'Weaknesses': 'Can be slow with large datasets, less interpretable than single trees'\n",
    "    },\n",
    "    'Non-Linear (KNN)': {\n",
    "        'Full Name': 'K-Nearest Neighbors',\n",
    "        'Category': 'Non-Linear (Instance-based)',\n",
    "        'Characteristics': 'Classifies based on majority vote of k nearest neighbors in feature space',\n",
    "        'Strengths': 'Simple, no training phase, naturally handles multi-class',\n",
    "        'Weaknesses': 'Slow prediction, sensitive to irrelevant features and scaling'\n",
    "    },\n",
    "    'Ensemble - GB': {\n",
    "        'Full Name': 'Gradient Boosting',\n",
    "        'Category': 'Ensemble (Boosting)',\n",
    "        'Characteristics': 'Sequentially builds weak learners, each correcting predecessor errors',\n",
    "        'Strengths': 'High accuracy, handles non-linear relationships, feature importance',\n",
    "        'Weaknesses': 'Prone to overfitting, slower training, sensitive to hyperparameters'\n",
    "    }\n",
    "}\n",
    "\n",
    "for name, desc in algorithm_descriptions.items():\n",
    "    overview_data.append({\n",
    "        'Algorithm': name,\n",
    "        'Full Name': desc['Full Name'],\n",
    "        'Category': desc['Category'],\n",
    "        'Characteristics': desc['Characteristics']\n",
    "    })\n",
    "\n",
    "overview_df = pd.DataFrame(overview_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERVIEW OF SELECTED ALGORITHMS\")\n",
    "print(\"=\"*80)\n",
    "for _, row in overview_df.iterrows():\n",
    "    print(f\"\\n{row['Algorithm']} ({row['Full Name']})\")\n",
    "    print(f\"  Category: {row['Category']}\")\n",
    "    print(f\"  Characteristics: {row['Characteristics']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Performance Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframes\n",
    "baseline_data = []\n",
    "optimised_data = []\n",
    "\n",
    "for name, result in all_results.items():\n",
    "    baseline_data.append({\n",
    "        'Classifier': name,\n",
    "        'Accuracy': result['baseline_metrics']['accuracy'],\n",
    "        'Precision': result['baseline_metrics']['precision'],\n",
    "        'Recall': result['baseline_metrics']['recall'],\n",
    "        'F1-Score': result['baseline_metrics']['f1'],\n",
    "        'MCC': result['baseline_metrics']['mcc'],\n",
    "        'AUC': result['baseline_auc']\n",
    "    })\n",
    "    optimised_data.append({\n",
    "        'Classifier': name,\n",
    "        'Accuracy': result['optimised_metrics']['accuracy'],\n",
    "        'Precision': result['optimised_metrics']['precision'],\n",
    "        'Recall': result['optimised_metrics']['recall'],\n",
    "        'F1-Score': result['optimised_metrics']['f1'],\n",
    "        'MCC': result['optimised_metrics']['mcc'],\n",
    "        'AUC': result['optimised_auc']\n",
    "    })\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_data)\n",
    "optimised_df = pd.DataFrame(optimised_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE MODELS PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "print(baseline_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMISED MODELS PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "print(optimised_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvement\n",
    "improvement_df = pd.DataFrame({\n",
    "    'Classifier': baseline_df['Classifier'],\n",
    "    'Accuracy': ((optimised_df['Accuracy'] - baseline_df['Accuracy']) / baseline_df['Accuracy'] * 100),\n",
    "    'Precision': ((optimised_df['Precision'] - baseline_df['Precision']) / baseline_df['Precision'] * 100),\n",
    "    'Recall': ((optimised_df['Recall'] - baseline_df['Recall']) / baseline_df['Recall'] * 100),\n",
    "    'F1-Score': ((optimised_df['F1-Score'] - baseline_df['F1-Score']) / baseline_df['F1-Score'] * 100),\n",
    "    'MCC': ((optimised_df['MCC'] - baseline_df['MCC']) / baseline_df['MCC'] * 100),\n",
    "    'AUC': ((optimised_df['AUC'] - baseline_df['AUC']) / baseline_df['AUC'] * 100)\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPROVEMENT AFTER OPTIMISATION (%)\")\n",
    "print(\"=\"*80)\n",
    "print(improvement_df.round(2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison of optimised models\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'MCC', 'AUC']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = ['#3498db', '#27ae60', '#9b59b6', '#e74c3c']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    values = optimised_df[metric].values\n",
    "    classifiers = optimised_df['Classifier'].values\n",
    "    \n",
    "    bars = ax.bar(classifiers, values, color=colors)\n",
    "    ax.set_title(metric, fontsize=14, fontweight='bold')\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.annotate(f'{val:.3f}', xy=(bar.get_x() + bar.get_width()/2, val),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\", ha='center', fontsize=9)\n",
    "\n",
    "plt.suptitle('Optimised Models Performance Comparison', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of optimised metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "heatmap_df = optimised_df.set_index('Classifier')[metrics]\n",
    "sns.heatmap(heatmap_df, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "            linewidths=0.5, vmin=0.5, vmax=1.0)\n",
    "plt.title('Performance Metrics Heatmap - Optimised Models', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart comparison\n",
    "from math import pi\n",
    "\n",
    "categories = metrics\n",
    "N = len(categories)\n",
    "\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "for idx, row in optimised_df.iterrows():\n",
    "    values = row[metrics].values.flatten().tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=row['Classifier'], color=colors[idx])\n",
    "    ax.fill(angles, values, alpha=0.1, color=colors[idx])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, size=12)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "plt.title('Radar Chart - Optimised Models Performance', fontsize=14, fontweight='bold', y=1.08)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline vs Optimised comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "classifiers = baseline_df['Classifier'].values\n",
    "x = np.arange(len(classifiers))\n",
    "width = 0.35\n",
    "\n",
    "metrics_to_plot = ['Accuracy', 'F1-Score', 'MCC', 'AUC']\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, baseline_df[metric], width, label='Baseline', color='#95a5a6')\n",
    "    bars2 = ax.bar(x + width/2, optimised_df[metric], width, label='Optimised', color='#2ecc71')\n",
    "    \n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title(f'{metric} - Baseline vs Optimised', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(classifiers, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    \n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\", ha='center', fontsize=8)\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\", ha='center', fontsize=8)\n",
    "\n",
    "plt.suptitle('Baseline vs Optimised Performance', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Optimisation Strategies Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMISATION STRATEGIES APPLIED BY EACH CLASSIFIER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, result in all_results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Strategies: {', '.join(result['optimisation_strategies'])}\")\n",
    "    print(f\"  Features: {result['n_features_original']} -> {result['n_features_selected']}\")\n",
    "    print(f\"  Feature reduction: {((result['n_features_original'] - result['n_features_selected']) / result['n_features_original'] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Ranking and Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank classifiers by each metric\n",
    "ranking_df = optimised_df.copy()\n",
    "ranking_df = ranking_df.set_index('Classifier')\n",
    "\n",
    "rankings = {}\n",
    "for metric in metrics:\n",
    "    rankings[metric] = ranking_df[metric].rank(ascending=False).astype(int)\n",
    "\n",
    "ranking_result = pd.DataFrame(rankings)\n",
    "ranking_result['Average Rank'] = ranking_result.mean(axis=1)\n",
    "ranking_result = ranking_result.sort_values('Average Rank')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASSIFIER RANKINGS (1 = Best)\")\n",
    "print(\"=\"*80)\n",
    "print(ranking_result.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best performing classifier\n",
    "best_classifier = ranking_result['Average Rank'].idxmin()\n",
    "best_metrics = optimised_df[optimised_df['Classifier'] == best_classifier].iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"BEST PERFORMING CLASSIFIER: {best_classifier}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Accuracy:  {best_metrics['Accuracy']:.4f}\")\n",
    "print(f\"  Precision: {best_metrics['Precision']:.4f}\")\n",
    "print(f\"  Recall:    {best_metrics['Recall']:.4f}\")\n",
    "print(f\"  F1-Score:  {best_metrics['F1-Score']:.4f}\")\n",
    "print(f\"  MCC:       {best_metrics['MCC']:.4f}\")\n",
    "print(f\"  AUC:       {best_metrics['AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Discussion and Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS AND DISCUSSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. ALGORITHM PERFORMANCE COMPARISON:\")\n",
    "print(\"   - Ensemble methods (Random Forest, Gradient Boosting) generally outperform\")\n",
    "print(\"     single classifiers due to their ability to combine multiple weak learners\")\n",
    "print(\"   - Linear classifiers (LDA) show competitive performance with faster training\")\n",
    "print(\"   - KNN provides good baseline but is computationally expensive for prediction\")\n",
    "\n",
    "print(\"\\n2. OPTIMISATION IMPACT:\")\n",
    "print(\"   - Hyperparameter tuning consistently improved all classifiers\")\n",
    "print(\"   - Feature selection reduced model complexity without significant performance loss\")\n",
    "print(\"   - Class weighting helped address the slight class imbalance in the dataset\")\n",
    "\n",
    "print(\"\\n3. METRIC ANALYSIS:\")\n",
    "print(\"   - For intrusion detection, Recall (detecting attacks) is crucial\")\n",
    "print(\"   - MCC provides balanced evaluation considering both classes\")\n",
    "print(\"   - AUC shows the trade-off between true positive and false positive rates\")\n",
    "\n",
    "print(\"\\n4. RECOMMENDATIONS FOR NETWORK INTRUSION DETECTION:\")\n",
    "print(\"   - Use ensemble methods for production systems requiring high accuracy\")\n",
    "print(\"   - Consider LDA for real-time detection where speed is critical\")\n",
    "print(\"   - Feature selection can significantly reduce computational requirements\")\n",
    "print(\"   - Regular model retraining is recommended as attack patterns evolve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary table\n",
    "summary_data = []\n",
    "\n",
    "for name, result in all_results.items():\n",
    "    summary_data.append({\n",
    "        'Classifier': name,\n",
    "        'Category': result['category'],\n",
    "        'Baseline F1': result['baseline_metrics']['f1'],\n",
    "        'Optimised F1': result['optimised_metrics']['f1'],\n",
    "        'F1 Improvement': ((result['optimised_metrics']['f1'] - result['baseline_metrics']['f1']) / \n",
    "                          result['baseline_metrics']['f1'] * 100),\n",
    "        'Optimised AUC': result['optimised_auc'],\n",
    "        'Features Used': result['n_features_selected'],\n",
    "        'Feature Reduction %': ((result['n_features_original'] - result['n_features_selected']) / \n",
    "                                result['n_features_original'] * 100)\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPREHENSIVE SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(summary_df.round(2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary to CSV for report\n",
    "summary_df.round(4).to_csv('results/group_summary.csv', index=False)\n",
    "baseline_df.round(4).to_csv('results/baseline_comparison.csv', index=False)\n",
    "optimised_df.round(4).to_csv('results/optimised_comparison.csv', index=False)\n",
    "\n",
    "print(\"\\nSummary tables saved to results folder:\")\n",
    "print(\"  - group_summary.csv\")\n",
    "print(\"  - baseline_comparison.csv\")\n",
    "print(\"  - optimised_comparison.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}