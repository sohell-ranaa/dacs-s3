{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Assignment: Linear Classifier\n",
    "## Network Intrusion Detection using Linear Discriminant Analysis (LDA)\n",
    "\n",
    "**Classifier Category:** Linear  \n",
    "**Algorithm:** Linear Discriminant Analysis  \n",
    "**Dataset:** NSL-KDD (Boosted Train + Preprocessed Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "data_path = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 3 (2680926479.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    from helpers import show_labels_dist, show_metrics, bias_var_metrics\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'if' statement on line 3\n"
     ]
    }
   ],
   "source": [
    "# Import helper functions\n",
    "from helpers import show_labels_dist, show_metrics, bias_var_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Boosted Train and Preprocessed Test datasets\n",
    "data_file = os.path.join(data_path, 'NSL_boosted-2.csv')\n",
    "train_df = pd.read_csv(data_file)\n",
    "print('Train Dataset: {} rows, {} columns'.format(train_df.shape[0], train_df.shape[1]))\n",
    "\n",
    "data_file = os.path.join(data_path, 'NSL_ppTest.csv')\n",
    "test_df = pd.read_csv(data_file)\n",
    "print('Test Dataset: {} rows, {} columns'.format(test_df.shape[0], test_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check numeric features consistency\n",
    "trnn = train_df.select_dtypes(include=['float64','int64']).columns\n",
    "tstn = test_df.select_dtypes(include=['float64','int64']).columns\n",
    "trndif = np.setdiff1d(trnn, tstn)\n",
    "tstdif = np.setdiff1d(tstn, trnn)\n",
    "\n",
    "print(\"Numeric features in train_set not in test_set: \", 'None' if len(trndif) == 0 else trndif)\n",
    "print(\"Numeric features in test_set not in train_set: \", 'None' if len(tstdif) == 0 else tstdif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check categorical features consistency\n",
    "trnn = train_df.select_dtypes(include=['object']).columns\n",
    "tstn = test_df.select_dtypes(include=['object']).columns\n",
    "print(\"Categorical features in train:\", trnn.tolist())\n",
    "print(\"Categorical features in test:\", tstn.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print('Missing Values - Train Set:', train_df.isnull().sum().sum())\n",
    "print('Missing Values - Test Set:', test_df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets for consistent preprocessing\n",
    "combined_df = pd.concat([train_df, test_df])\n",
    "print('Combined Dataset: {} rows, {} columns'.format(combined_df.shape[0], combined_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check label distributions\n",
    "print(\"Label distribution in combined dataset:\")\n",
    "print(combined_df['label'].value_counts())\n",
    "print(\"\\nAttack category distribution:\")\n",
    "print(combined_df['atakcat'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set classification target (Two-class: normal vs attack)\n",
    "twoclass = True\n",
    "\n",
    "if twoclass:\n",
    "    labels_df = combined_df['label'].copy()\n",
    "    labels_df[labels_df != 'normal'] = 'attack'\n",
    "else:\n",
    "    labels_df = combined_df[['atakcat']].copy()\n",
    "    labels_df.rename(columns={'atakcat':'label'}, inplace=True)\n",
    "    labels_df = labels_df.squeeze('columns')\n",
    "\n",
    "# Drop target features\n",
    "combined_df.drop(['label'], axis=1, inplace=True)\n",
    "combined_df.drop(['atakcat'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding categorical features\n",
    "categori = combined_df.select_dtypes(include=['object']).columns\n",
    "category_cols = categori.tolist()\n",
    "print(\"Categorical columns to encode:\", category_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.get_dummies(combined_df, columns=category_cols)\n",
    "print('Features after encoding: {} columns'.format(features_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numeric columns for scaling\n",
    "numeri = combined_df.select_dtypes(include=['float64','int64']).columns\n",
    "print(\"Numeric columns for scaling:\", numeri.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore train/test split\n",
    "X_train = features_df.iloc[:len(train_df),:].copy()\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "X_test = features_df.iloc[len(train_df):,:].copy()\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "y_train = labels_df[:len(train_df)]\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "y_test = labels_df[len(train_df):]\n",
    "y_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MinMaxScaler (fit on train, transform both)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "for i in numeri:\n",
    "    arr = np.array(X_train[i])\n",
    "    scale = MinMaxScaler().fit(arr.reshape(-1, 1))\n",
    "    X_train[i] = scale.transform(arr.reshape(len(arr),1))\n",
    "    \n",
    "    arr = np.array(X_test[i])\n",
    "    X_test[i] = scale.transform(arr.reshape(len(arr),1))\n",
    "\n",
    "print(\"Scaling completed using MinMaxScaler (0-1 range)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save original datasets before optimization\n",
    "X_train_original = X_train.copy()\n",
    "X_test_original = X_test.copy()\n",
    "y_train_original = y_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. BASELINE MODEL: Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Create baseline model with default parameters\n",
    "baseline_model = LinearDiscriminantAnalysis()\n",
    "print(\"Baseline Model:\", baseline_model)\n",
    "print(\"\\nDefault Parameters:\", baseline_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show label distribution\n",
    "show_labels_dist(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate baseline model\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trs = time()\n",
    "baseline_model.fit(X_train, y_train)\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "tre = time() - trs\n",
    "\n",
    "print(f\"Training Time: {tre:.2f} seconds\\n\")\n",
    "show_metrics(y_test, y_pred_baseline, baseline_model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias-Variance Decomposition for baseline\n",
    "print(\"\\nBias-Variance Decomposition (Baseline):\")\n",
    "bias_var_metrics(X_train, X_test, y_train, y_test, LinearDiscriminantAnalysis(), folds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store baseline metrics for comparison\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "baseline_metrics = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_baseline),\n",
    "    'precision': precision_score(y_test, y_pred_baseline, pos_label='attack'),\n",
    "    'recall': recall_score(y_test, y_pred_baseline, pos_label='attack'),\n",
    "    'f1': f1_score(y_test, y_pred_baseline, pos_label='attack'),\n",
    "    'mcc': matthews_corrcoef(y_test, y_pred_baseline)\n",
    "}\n",
    "print(\"Baseline Metrics Stored:\", baseline_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. OPTIMISATION STRATEGY 1: Hyperparameter Tuning with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# Define parameter grid for LDA\n",
    "param_grid = {\n",
    "    'solver': ['svd', 'lsqr', 'eigen'],\n",
    "    'shrinkage': [None, 'auto', 0.1, 0.5, 0.9],  # Only for lsqr and eigen\n",
    "}\n",
    "\n",
    "# Note: shrinkage only works with 'lsqr' or 'eigen' solvers\n",
    "# We'll do a more careful grid search\n",
    "\n",
    "print(\"Hyperparameter Grid Search for LDA\")\n",
    "print(\"Parameters to tune: solver, shrinkage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search with different configurations\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "results = []\n",
    "configs = [\n",
    "    {'solver': 'svd', 'shrinkage': None},\n",
    "    {'solver': 'lsqr', 'shrinkage': None},\n",
    "    {'solver': 'lsqr', 'shrinkage': 'auto'},\n",
    "    {'solver': 'lsqr', 'shrinkage': 0.1},\n",
    "    {'solver': 'lsqr', 'shrinkage': 0.5},\n",
    "    {'solver': 'lsqr', 'shrinkage': 0.9},\n",
    "    {'solver': 'eigen', 'shrinkage': None},\n",
    "    {'solver': 'eigen', 'shrinkage': 'auto'},\n",
    "    {'solver': 'eigen', 'shrinkage': 0.1},\n",
    "    {'solver': 'eigen', 'shrinkage': 0.5},\n",
    "]\n",
    "\n",
    "print(\"Testing configurations with 5-fold Cross-Validation...\\n\")\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for config in configs:\n",
    "    try:\n",
    "        model = LinearDiscriminantAnalysis(**config)\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='f1_weighted', n_jobs=-1)\n",
    "        results.append({\n",
    "            'config': config,\n",
    "            'mean_score': scores.mean(),\n",
    "            'std_score': scores.std()\n",
    "        })\n",
    "        print(f\"{config} -> F1: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "    except Exception as e:\n",
    "        print(f\"{config} -> Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best configuration\n",
    "best_result = max(results, key=lambda x: x['mean_score'])\n",
    "print(f\"\\nBest Configuration: {best_result['config']}\")\n",
    "print(f\"Best CV F1 Score: {best_result['mean_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. OPTIMISATION STRATEGY 2: Feature Selection via Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix for numeric features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode target for correlation\n",
    "y_encoded = LabelEncoder().fit_transform(y_train)\n",
    "\n",
    "# Create dataframe with features and encoded target\n",
    "corr_df = X_train.copy()\n",
    "corr_df['target'] = y_encoded\n",
    "\n",
    "# Calculate correlation with target\n",
    "correlations = corr_df.corr()['target'].drop('target').abs().sort_values(ascending=False)\n",
    "print(\"Top 20 features correlated with target:\")\n",
    "print(correlations.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top correlations\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = correlations.head(25)\n",
    "sns.barplot(x=top_features.values, y=top_features.index, palette='viridis')\n",
    "plt.title('Top 25 Features by Correlation with Target')\n",
    "plt.xlabel('Absolute Correlation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top correlated features (threshold > 0.1)\n",
    "threshold = 0.1\n",
    "selected_features = correlations[correlations > threshold].index.tolist()\n",
    "print(f\"\\nSelected {len(selected_features)} features with correlation > {threshold}\")\n",
    "print(selected_features[:10], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reduced datasets\n",
    "X_train_reduced = X_train[selected_features]\n",
    "X_test_reduced = X_test[selected_features]\n",
    "print(f\"Reduced feature set: {X_train_reduced.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. OPTIMISED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimised model with best parameters and reduced features\n",
    "optimised_model = LinearDiscriminantAnalysis(**best_result['config'])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"OPTIMISED MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Parameters: {best_result['config']}\")\n",
    "print(f\"Features: {len(selected_features)} (reduced from {X_train.shape[1]})\")\n",
    "\n",
    "trs = time()\n",
    "optimised_model.fit(X_train_reduced, y_train)\n",
    "y_pred_optimised = optimised_model.predict(X_test_reduced)\n",
    "tre = time() - trs\n",
    "\n",
    "print(f\"\\nTraining Time: {tre:.2f} seconds\\n\")\n",
    "show_metrics(y_test, y_pred_optimised, optimised_model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias-Variance Decomposition for optimised model\n",
    "print(\"\\nBias-Variance Decomposition (Optimised):\")\n",
    "bias_var_metrics(X_train_reduced, X_test_reduced, y_train, y_test, \n",
    "                 LinearDiscriminantAnalysis(**best_result['config']), folds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store optimised metrics\n",
    "optimised_metrics = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_optimised),\n",
    "    'precision': precision_score(y_test, y_pred_optimised, pos_label='attack'),\n",
    "    'recall': recall_score(y_test, y_pred_optimised, pos_label='attack'),\n",
    "    'f1': f1_score(y_test, y_pred_optimised, pos_label='attack'),\n",
    "    'mcc': matthews_corrcoef(y_test, y_pred_optimised)\n",
    "}\n",
    "print(\"Optimised Metrics:\", optimised_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. COMPARISON: Baseline vs Optimised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'MCC'],\n",
    "    'Baseline': [baseline_metrics['accuracy'], baseline_metrics['precision'], \n",
    "                 baseline_metrics['recall'], baseline_metrics['f1'], baseline_metrics['mcc']],\n",
    "    'Optimised': [optimised_metrics['accuracy'], optimised_metrics['precision'],\n",
    "                  optimised_metrics['recall'], optimised_metrics['f1'], optimised_metrics['mcc']]\n",
    "})\n",
    "comparison_df['Improvement'] = comparison_df['Optimised'] - comparison_df['Baseline']\n",
    "comparison_df['Improvement %'] = (comparison_df['Improvement'] / comparison_df['Baseline'] * 100).round(2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE COMPARISON: BASELINE vs OPTIMISED\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(comparison_df['Metric']))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, comparison_df['Baseline'], width, label='Baseline', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, comparison_df['Optimised'], width, label='Optimised', color='darkorange')\n",
    "\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Linear Discriminant Analysis: Baseline vs Optimised')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Metric'])\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=8)\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Comparison\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Baseline confusion matrix\n",
    "cm_baseline = confusion_matrix(y_test, y_pred_baseline, labels=baseline_model.classes_)\n",
    "disp1 = ConfusionMatrixDisplay(confusion_matrix=cm_baseline, display_labels=baseline_model.classes_)\n",
    "disp1.plot(ax=axes[0], cmap='Blues')\n",
    "axes[0].set_title('Baseline Model')\n",
    "\n",
    "# Optimised confusion matrix\n",
    "cm_optimised = confusion_matrix(y_test, y_pred_optimised, labels=optimised_model.classes_)\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm_optimised, display_labels=optimised_model.classes_)\n",
    "disp2.plot(ax=axes[1], cmap='Oranges')\n",
    "axes[1].set_title('Optimised Model')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve Comparison\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Get probabilities\n",
    "y_prob_baseline = baseline_model.predict_proba(X_test)[:, 1]\n",
    "y_prob_optimised = optimised_model.predict_proba(X_test_reduced)[:, 1]\n",
    "\n",
    "# Convert labels to binary\n",
    "y_test_binary = (y_test == 'attack').astype(int)\n",
    "\n",
    "# Calculate ROC curves\n",
    "fpr_base, tpr_base, _ = roc_curve(y_test_binary, y_prob_baseline)\n",
    "fpr_opt, tpr_opt, _ = roc_curve(y_test_binary, y_prob_optimised)\n",
    "\n",
    "auc_base = auc(fpr_base, tpr_base)\n",
    "auc_opt = auc(fpr_opt, tpr_opt)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_base, tpr_base, 'b-', label=f'Baseline (AUC = {auc_base:.4f})')\n",
    "plt.plot(fpr_opt, tpr_opt, 'r-', label=f'Optimised (AUC = {auc_opt:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison - Linear Discriminant Analysis')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SUMMARY: LINEAR DISCRIMINANT ANALYSIS FOR INTRUSION DETECTION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n1. CLASSIFIER CATEGORY: Linear\")\n",
    "print(\"   Algorithm: Linear Discriminant Analysis (LDA)\")\n",
    "print(\"\\n2. OPTIMISATION STRATEGIES APPLIED:\")\n",
    "print(\"   a) Hyperparameter Tuning with Cross-Validation\")\n",
    "print(f\"      - Best solver: {best_result['config']['solver']}\")\n",
    "print(f\"      - Best shrinkage: {best_result['config']['shrinkage']}\")\n",
    "print(\"   b) Feature Selection via Correlation Analysis\")\n",
    "print(f\"      - Original features: {X_train.shape[1]}\")\n",
    "print(f\"      - Selected features: {len(selected_features)}\")\n",
    "print(f\"      - Feature reduction: {((X_train.shape[1] - len(selected_features)) / X_train.shape[1] * 100):.1f}%\")\n",
    "print(\"\\n3. PERFORMANCE IMPROVEMENT:\")\n",
    "for _, row in comparison_df.iterrows():\n",
    "    print(f\"   {row['Metric']}: {row['Baseline']:.4f} -> {row['Optimised']:.4f} ({row['Improvement %']:+.2f}%)\")\n",
    "print(f\"\\n4. ROC-AUC: {auc_base:.4f} -> {auc_opt:.4f}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for group comparison\n",
    "results_dict = {\n",
    "    'classifier': 'Linear Discriminant Analysis',\n",
    "    'category': 'Linear',\n",
    "    'baseline_metrics': baseline_metrics,\n",
    "    'optimised_metrics': optimised_metrics,\n",
    "    'baseline_auc': auc_base,\n",
    "    'optimised_auc': auc_opt,\n",
    "    'optimisation_strategies': ['Hyperparameter Tuning', 'Feature Selection (Correlation)'],\n",
    "    'best_params': best_result['config'],\n",
    "    'n_features_original': X_train.shape[1],\n",
    "    'n_features_selected': len(selected_features)\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "import json\n",
    "with open('results/linear_lda_results.json', 'w') as f:\n",
    "    json.dump(results_dict, f, indent=2)\n",
    "print(\"Results saved to: results/linear_lda_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}