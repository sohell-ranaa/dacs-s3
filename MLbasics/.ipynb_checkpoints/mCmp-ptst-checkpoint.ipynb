{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **tomato juice dataset**\n",
    "<br>` 'quality' is the target feature for classification `\n",
    "<br>` the other features are chemical properties of our product `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_S34U5S-i69d"
   },
   "source": [
    "**Import the main libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "_import the local library_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add parent folder path where lib folder is\n",
    "import sys\n",
    "if \"..\" not in sys.path:import sys; sys.path.insert(0, '..') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "from mylib import show_labels_dist, show_metrics, bias_var_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9IZetEZ8jQJm"
   },
   "source": [
    "**Import the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions: 1599 rows, 12 columns\n"
     ]
    }
   ],
   "source": [
    "## file path: unix style\n",
    "df = pd.read_csv('../datasets/tomatjus.csv')\n",
    "\n",
    "# shape method gives the dimensions of the dataset\n",
    "print('Dataset dimensions: {} rows, {} columns'.format(\n",
    "    df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         1599 non-null   float64\n",
      " 1   volatile acidity      1599 non-null   float64\n",
      " 2   citric acid           1599 non-null   float64\n",
      " 3   residual sugar        1599 non-null   float64\n",
      " 4   chlorides             1599 non-null   float64\n",
      " 5   free sulfur dioxide   1599 non-null   float64\n",
      " 6   total sulfur dioxide  1599 non-null   float64\n",
      " 7   density               1599 non-null   float64\n",
      " 8   pH                    1599 non-null   float64\n",
      " 9   sulphates             1599 non-null   float64\n",
      " 10  pulp                  1599 non-null   float64\n",
      " 11  quality               1599 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "***\n",
    "**Data Preparation and EDA** (unique to this dataset)\n",
    "* _Check for missing values_\n",
    "* _Quick visual check of unique values_\n",
    "* _Split the classification feature out of the dataset_\n",
    "* _Check column names of categorical attributes ( for get_dummies() )_\n",
    "* _Check column names of numeric attributes ( for Scaling )_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Let's skip the checking_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification target feature**\n",
    "<br>_Make it a multi-class problem, using text labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  divide into classes by giving a range for quality\n",
    "##  Make it a multi-class problem: {3,4,5} {6} {7.8}\n",
    "bins = (2, 5, 6, 8)\n",
    "group_names = ['Average', 'Premium', 'Special']\n",
    "df['quality'] = pd.cut(df['quality'], bins = bins, labels = group_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split the classification feature out of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature being predicted (\"the Right Answer\")\n",
    "labels_col = 'quality'\n",
    "y = df[labels_col]\n",
    "\n",
    "## Features used for prediction \n",
    "# pandas has a lot of rules about returning a 'view' vs. a copy from slice\n",
    "# so we force it to create a new dataframe \n",
    "X = df.copy()\n",
    "X.drop(labels_col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "***\n",
    "**<br>Create Test // Train Datasets**\n",
    "> Split X and y datasets into Train and Test subsets,<br>keeping relative proportions of each class (stratify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=50, \n",
    "                                                    stratify=y)\n",
    "# train_test_split does random selection, \n",
    "#      so we should reset the dataframe indexes\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "y_test.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**<br>Scaling** comes _after_ test // train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'pulp']\n"
     ]
    }
   ],
   "source": [
    "numeri = X.select_dtypes(include=['float64','int64']).columns\n",
    "print(numeri.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the Numeric columns \n",
    "# StandardScaler range: -1 to 1, MinMaxScaler range: zero to 1\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# sklearn docs say \n",
    "#   \"Don't cheat - fit only on training data, then transform both\"\n",
    "#   fit() expects 2D array: reshape(-1, 1) for single col or (1, -1) single row\n",
    "\n",
    "for i in numeri:\n",
    "    arr = np.array(X_train[i])\n",
    "    scale = MinMaxScaler().fit(arr.reshape(-1, 1))\n",
    "    X_train[i] = scale.transform(arr.reshape(len(arr),1))\n",
    "\n",
    "    arr = np.array(X_test[i])\n",
    "    X_test[i] = scale.transform(arr.reshape(len(arr),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**<br>Classifier Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('LogReg', LogisticRegression()), ('LinearDA', LinearDiscriminantAnalysis())]\n"
     ]
    }
   ],
   "source": [
    "# prepare list\n",
    "models = []\n",
    "\n",
    "##  --  Linear  --  ## \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "models.append ((\"LogReg\",LogisticRegression())) \n",
    "#from sklearn.linear_model import SGDClassifier \n",
    "#models.append ((\"StocGradDes\",SGDClassifier())) \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "models.append((\"LinearDA\", LinearDiscriminantAnalysis())) \n",
    "#from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis \n",
    "#models.append((\"QuadraticDA\", QuadraticDiscriminantAnalysis())) \n",
    "\n",
    "##  --  Support Vector  --  ## \n",
    "#from sklearn.svm import SVC \n",
    "#models.append((\"SupportVectorClf\", SVC())) \n",
    "#from sklearn.svm import LinearSVC \n",
    "#models.append((\"LinearSVC\", LinearSVC())) \n",
    "#from sklearn.linear_model import RidgeClassifier\n",
    "#models.append ((\"RidgeClf\",RidgeClassifier())) \n",
    "\n",
    "##  --  Non-linear  --  ## \n",
    "#from sklearn.tree import DecisionTreeClassifier \n",
    "#models.append ((\"DecisionTree\",DecisionTreeClassifier())) \n",
    "#from sklearn.naive_bayes import GaussianNB \n",
    "#models.append ((\"GaussianNB\",GaussianNB())) \n",
    "#from sklearn.neighbors import KNeighborsClassifier \n",
    "#models.append((\"K-NNeighbors\", KNeighborsClassifier())) \n",
    "\n",
    "##  --  Ensemble: bagging  --  ## \n",
    "#from sklearn.ensemble import RandomForestClassifier \n",
    "#models.append((\"RandomForest\", RandomForestClassifier())) \n",
    "##  --  Ensemble: boosting  --  ## \n",
    "#from sklearn.ensemble import AdaBoostClassifier \n",
    "#models.append((\"AdaBoost\", AdaBoostClassifier())) \n",
    "#from sklearn.ensemble import GradientBoostingClassifier \n",
    "#models.append((\"GradientBoost\", GradientBoostingClassifier())) \n",
    "\n",
    "##  --  NeuralNet (simplest)  --  ## \n",
    "#from sklearn.neural_network import MLPClassifier \n",
    "#models.append((\"MultiLayerPtron\", MLPClassifier())) \n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**<br>Target Label Distributions** (standard block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_train: 1279 rows, 11 columns\n",
      "features_test:  320 rows, 11 columns\n",
      "\n",
      "labels_train: 1279 rows, 1 column\n",
      "labels_test:  320 rows, 1 column\n",
      "\n",
      "Frequency and Distribution of labels\n",
      "         quality  %_train  quality  %_test\n",
      "quality                                   \n",
      "Average      595    46.52      149   46.56\n",
      "Premium      510    39.87      128   40.00\n",
      "Special      174    13.60       43   13.44\n"
     ]
    }
   ],
   "source": [
    "# from our local library\n",
    "show_labels_dist(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "**<br>Fit and Predict** (standard block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro average: unweighted mean per label\n",
      "weighted average: support-weighted mean per label\n",
      "MCC: correlation between prediction and ground truth\n",
      "     (+1 perfect, 0 random prediction, -1 inverse)\n",
      "\n",
      "Confusion Matrix: LogReg\n",
      "Run Time 0.29 seconds\n",
      "\n",
      "               pred:Average  pred:Premium  pred:Special\n",
      "train:Average           120            29             0\n",
      "train:Premium            44            78             6\n",
      "train:Special             2            29            12\n",
      "\n",
      "~~~~\n",
      "     Average :  FPR = 0.269   FNR = 0.195\n",
      "     Premium :  FPR = 0.302   FNR = 0.391\n",
      "     Special :  FPR = 0.022   FNR = 0.721\n",
      "\n",
      "   macro avg :  FPR = 0.198   FNR = 0.435\n",
      "weighted avg :  FPR = 0.172   FNR = 0.344\n",
      "\n",
      "~~~~\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Average      0.723     0.805     0.762       149\n",
      "     Premium      0.574     0.609     0.591       128\n",
      "     Special      0.667     0.279     0.393        43\n",
      "\n",
      "    accuracy                          0.656       320\n",
      "   macro avg      0.654     0.565     0.582       320\n",
      "weighted avg      0.656     0.656     0.644       320\n",
      "\n",
      "~~~~\n",
      "MCC: Overall :  0.412\n",
      "     Average :  0.535\n",
      "     Premium :  0.305\n",
      "     Special :  0.381\n",
      "\n",
      "Parameters:  {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False} \n",
      "\n",
      "\n",
      "Confusion Matrix: LinearDA\n",
      "Run Time 0.08 seconds\n",
      "\n",
      "               pred:Average  pred:Premium  pred:Special\n",
      "train:Average           116            31             2\n",
      "train:Premium            40            76            12\n",
      "train:Special             2            19            22\n",
      "\n",
      "~~~~\n",
      "     Average :  FPR = 0.246   FNR = 0.221\n",
      "     Premium :  FPR = 0.260   FNR = 0.406\n",
      "     Special :  FPR = 0.051   FNR = 0.488\n",
      "\n",
      "   macro avg :  FPR = 0.186   FNR = 0.372\n",
      "weighted avg :  FPR = 0.166   FNR = 0.331\n",
      "\n",
      "~~~~\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Average      0.734     0.779     0.756       149\n",
      "     Premium      0.603     0.594     0.598       128\n",
      "     Special      0.611     0.512     0.557        43\n",
      "\n",
      "    accuracy                          0.669       320\n",
      "   macro avg      0.649     0.628     0.637       320\n",
      "weighted avg      0.665     0.669     0.666       320\n",
      "\n",
      "~~~~\n",
      "MCC: Overall :  0.446\n",
      "     Average :  0.532\n",
      "     Premium :  0.334\n",
      "     Special :  0.498\n",
      "\n",
      "Parameters:  {'covariance_estimator': None, 'n_components': None, 'priors': None, 'shrinkage': None, 'solver': 'svd', 'store_covariance': False, 'tol': 0.0001} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "\n",
    "print('macro average: unweighted mean per label')\n",
    "print('weighted average: support-weighted mean per label')\n",
    "print('MCC: correlation between prediction and ground truth')\n",
    "print('     (+1 perfect, 0 random prediction, -1 inverse)\\n')\n",
    "\n",
    "for name, clf in models:\n",
    "    trs = time()\n",
    "    print('Confusion Matrix:', name)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    ygx = clf.predict(X_test)\n",
    "    results.append((name, ygx))\n",
    "    \n",
    "    tre = time() - trs\n",
    "    print (\"Run Time {} seconds\".format(round(tre,2)) + '\\n')\n",
    "    \n",
    "# Easy way to ensure that the confusion matrix rows and columns\n",
    "#   are labeled exactly as the classifier has coded the classes\n",
    "#   [[note the _ at the end of clf.classes_ ]]\n",
    "\n",
    "    show_metrics(y_test, ygx, clf.classes_)   # from our local library\n",
    "    print('\\nParameters: ', clf.get_params(), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**Bias - Variance Decomposition** (standard block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias // Variance Decomposition: LogReg\n",
      "   Average bias: 0.344\n",
      "   Average variance: 0.064\n",
      "   Average expected loss: 0.352  \"Goodness\": 0.648\n",
      "\n",
      "Bias // Variance Decomposition: LinearDA\n",
      "   Average bias: 0.325\n",
      "   Average variance: 0.072\n",
      "   Average expected loss: 0.340  \"Goodness\": 0.660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from our local library\n",
    "# reduce (cross-validation) folds for faster results\n",
    "folds = 20\n",
    "for name, clf in models:\n",
    "    print('Bias // Variance Decomposition:', name)\n",
    "    bias_var_metrics(X_train,X_test,y_train,y_test,clf,folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "***\n",
    "**Statistical Comparison of Models**\n",
    "<br>The the null hypothesis statement:\n",
    ">H0: Both models perform equally well on the dataset.\n",
    "<br>H1: Both models do not have the same performance on the dataset.\n",
    "\n",
    "Chosen significance threshold is 0.05 (a=0.05) for rejecting the null hypothesis.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the results from the first test run\n",
    "#     (make sure these match up)\n",
    "\n",
    "# ('LogReg', LogisticRegression())\n",
    "model1 = models[0][1]\n",
    "scores1 = results[0][1]\n",
    "\n",
    "# ('LinearDA', LinearDiscriminantAnalysis())\n",
    "model2 = models[1][1]\n",
    "scores2 = results[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wilcoxon signed-rank test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA and LR - Wilcoxon Test: p value = 0.007\n",
      "Since p < 0.05, we can reject the null-hypothesis that\n",
      "                both models perform equally well on this dataset.\n",
      "We may conclude that the difference in performance is significant.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# this one requires numeric labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "ns1 = LabelEncoder().fit_transform(scores1)\n",
    "ns2 = LabelEncoder().fit_transform(scores2)\n",
    "\n",
    "s, pv = wilcoxon(ns1, ns2, zero_method='zsplit')\n",
    "\n",
    "print('LDA and LR - Wilcoxon Test: p value =', round(pv,3))\n",
    "\n",
    "if pv <= 0.05:\n",
    "    print('Since p < 0.05, we can reject the null-hypothesis that')\n",
    "    print('                both models perform equally well on this dataset.')\n",
    "    print('We may conclude that the difference in performance is significant.')\n",
    "else:\n",
    "    print('Since p > 0.05, we cannot reject the null hypothesis that')\n",
    "    print('                both models perform equally well on this dataset.')\n",
    "    print('We must conclude that the difference in performance is not significant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* McNemar's test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA and LR - McNemar Test: p value = 0.541 and ChiSquare = None\n",
      "Since p > 0.05, we cannot reject the null hypothesis that\n",
      "                both models perform equally well on this dataset.\n",
      "We must conclude that the difference in performance is not significant\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.evaluate import mcnemar_table, mcnemar\n",
    "\n",
    "tb = mcnemar_table(y_target=y_test, \n",
    "                   y_model1=scores1, \n",
    "                   y_model2=scores2)\n",
    "chisq, pv = mcnemar(ary=tb, exact=True)\n",
    "\n",
    "print('LDA and LR - McNemar Test: p value =', round(pv,3), 'and ChiSquare =', chisq)\n",
    "\n",
    "if pv <= 0.05:\n",
    "    print('Since p < 0.05, we can reject the null-hypothesis that')\n",
    "    print('                both models perform equally well on this dataset.')\n",
    "    print('We may conclude that the difference in performance is significant.')\n",
    "else:\n",
    "    print('Since p > 0.05, we cannot reject the null hypothesis that')\n",
    "    print('                both models perform equally well on this dataset.')\n",
    "    print('We must conclude that the difference in performance is not significant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 5x2CV paired t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA and LR - 5x2CV paired t-test: p value = 0.118 and t value = -1.884\n",
      "Since p > 0.05, we cannot reject the null hypothesis that\n",
      "                both models perform equally well on this dataset.\n",
      "We must conclude that the difference in performance is not significant\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.evaluate import paired_ttest_5x2cv\n",
    "\n",
    "# Calculate p-value using previous k-Fold classifier variables\n",
    "t, pv = paired_ttest_5x2cv(estimator1=model1,\n",
    "                          estimator2=model2,\n",
    "                          X=X_train, y=y_train,\n",
    "                          random_seed=11)\n",
    "\n",
    "print('LDA and LR - 5x2CV paired t-test: p value =', round(pv,3), 'and t value =', round(t,3))\n",
    "\n",
    "if pv <= 0.05:\n",
    "    print('Since p < 0.05, we can reject the null-hypothesis that')\n",
    "    print('                both models perform equally well on this dataset.')\n",
    "    print('We may conclude that the difference in performance is significant.')\n",
    "else:\n",
    "    print('Since p > 0.05, we cannot reject the null hypothesis that')\n",
    "    print('                both models perform equally well on this dataset.')\n",
    "    print('We must conclude that the difference in performance is not significant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 5x2CV combined F test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA and LR - 5x2CV Combined f Test: p value = 0.317 and f value = 1.592\n",
      "Since p > 0.05, we cannot reject the null hypothesis that\n",
      "                both models perform equally well on this dataset.\n",
      "We must conclude that the difference in performance is not significant\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.evaluate import combined_ftest_5x2cv\n",
    "\n",
    "# Calculate p-value using previous k-Fold classifier variables\n",
    "f, pv = combined_ftest_5x2cv(estimator1=model1,\n",
    "                            estimator2=model2,\n",
    "                            X=X_train, y=y_train,\n",
    "                            random_seed=11)\n",
    "\n",
    "print('LDA and LR - 5x2CV Combined f Test: p value =', round(pv,3), 'and f value =', round(f,3))\n",
    "\n",
    "if pv <= 0.05:\n",
    "    print('Since p < 0.05, we can reject the null-hypothesis that')\n",
    "    print('                both models perform equally well on this dataset.')\n",
    "    print('We may conclude that the difference in performance is significant.')\n",
    "else:\n",
    "    print('Since p > 0.05, we cannot reject the null hypothesis that')\n",
    "    print('                both models perform equally well on this dataset.')\n",
    "    print('We must conclude that the difference in performance is not significant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
