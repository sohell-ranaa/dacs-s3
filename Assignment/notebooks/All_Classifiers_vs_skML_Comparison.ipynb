{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Comprehensive Comparison: Professor's Template vs Our Implementations\n\n**Purpose:** Compare the given script (skML-complete) with all three classifier implementations.\n\n**Group Members:**\n- Muhammad Usama Fazal (TP086008) - Linear Classifier (LDA)\n- Imran Shahadat Noble (TP087895) - Ensemble Classifier (Random Forest)\n- Md Sohel Rana (TP087437) - Non-Linear Classifier (KNN)\n\n| Classifier | Notebook | Member | Best Model | MCC |\n|------------|----------|--------|------------|-----|\n| Linear | 01_Linear_Classifier.ipynb | Muhammad Usama Fazal (TP086008) | LDA | 0.671 |\n| Ensemble | 02_Ensemble_Classifier.ipynb | Imran Shahadat Noble (TP087895) | Random Forest | 0.815 |\n| Non-Linear | 03_NonLinear_Classifier.ipynb | Md Sohel Rana (TP087437) | KNN | 0.816 |\n\n**Data Verified:** 2024-12-13\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Structure Comparison: Template vs All Implementations\n\n| Aspect | skML-complete (Template) | Linear (LDA) | Ensemble (RF) | Non-Linear (KNN) |\n|--------|-------------------------|--------------|---------------|------------------|\n| **Header** | Version info only | Full header | Full header | Full header |\n| **Classification** | 2-class | 5-class | 5-class | 5-class |\n| **Baseline Models** | Multiple (commented) | LDA, LogReg, Ridge | RF, ExtraTrees, AdaBoost | KNN, DT, SVM |\n| **Best Baseline** | Not specified | LDA | Random Forest | KNN |\n| **Optimization 1** | Template only | Hyperparameter Tuning | RandomizedSearchCV | GridSearchCV |\n| **Optimization 2** | Not included | Correlation Filter | Feature Importance | Correlation Filter |\n| **Features Used** | 122 (all) | 30 (reduced) | 37 (reduced) | 30 (reduced) |\n| **Visualizations** | None | Yes | Yes | Yes |\n| **Results Export** | None | JSON + CSV | JSON + CSV | JSON + CSV |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Code Comparison by Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Classification Target Setting\n",
    "\n",
    "**skML-complete (Template):**\n",
    "```python\n",
    "twoclass = True     # 2-class: normal vs attack\n",
    "```\n",
    "\n",
    "**All Our Implementations:**\n",
    "```python\n",
    "twoclass = False    # 5-class: benign, dos, probe, r2l, u2r\n",
    "```\n",
    "\n",
    "**Reason:** Assignment requires MCC per attack class, which needs 5-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model Selection Comparison\n",
    "\n",
    "**skML-complete (Template):**\n",
    "```python\n",
    "models = []\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "models.append((\"LinearDA\", LinearDiscriminantAnalysis()))\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "models.append((\"GaussianNB\", GaussianNB()))\n",
    "# Other models commented out...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**01_Linear_Classifier.ipynb:**\n",
    "```python\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "\n",
    "lda_baseline = LinearDiscriminantAnalysis()\n",
    "lr_baseline = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "ridge_baseline = RidgeClassifier(class_weight='balanced')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**02_Ensemble_Classifier.ipynb:**\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "\n",
    "rf_baseline = RandomForestClassifier(n_estimators=100, class_weight='balanced')\n",
    "et_baseline = ExtraTreesClassifier(n_estimators=100, class_weight='balanced')\n",
    "ada_baseline = AdaBoostClassifier(n_estimators=100)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**03_NonLinear_Classifier.ipynb:**\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "knn_baseline = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "dt_baseline = DecisionTreeClassifier(class_weight='balanced')\n",
    "svm_baseline = SVC(kernel='rbf', class_weight='balanced')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Optimization Strategy Comparison\n",
    "\n",
    "| Classifier | Optimization 1 | Optimization 2 | Feature Reduction |\n",
    "|------------|----------------|----------------|-------------------|\n",
    "| **Template** | Template only (empty) | None | 0% |\n",
    "| **Linear (LDA)** | 5-fold CV tuning (solver, shrinkage) | Correlation filter (threshold=0.1) | 75.4% |\n",
    "| **Ensemble (RF)** | RandomizedSearchCV (10 iter, 3-fold) | Feature importance (95% cumulative) | 68.9% |\n",
    "| **Non-Linear (KNN)** | GridSearchCV (3-fold) | Correlation filter (threshold=0.1) | 75.4% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Hyperparameter Tuning Details\n",
    "\n",
    "#### Linear (LDA)\n",
    "```python\n",
    "configs = [\n",
    "    {'solver': 'svd', 'shrinkage': None},\n",
    "    {'solver': 'lsqr', 'shrinkage': 'auto'},\n",
    "    {'solver': 'lsqr', 'shrinkage': 0.1},\n",
    "    # ... more configs\n",
    "]\n",
    "# Best: solver='svd', shrinkage=None\n",
    "```\n",
    "\n",
    "#### Ensemble (Random Forest)\n",
    "```python\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150],\n",
    "    'max_depth': [20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt'],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "# Best: n_estimators=100, max_depth=None, min_samples_split=2\n",
    "```\n",
    "\n",
    "#### Non-Linear (KNN)\n",
    "```python\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2],  # Manhattan vs Euclidean\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "# Best: n_neighbors=3, weights='distance', p=1 (Manhattan)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Performance Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Baseline Comparison (Best from each category) - VERIFIED DATA\nbaseline_results = pd.DataFrame({\n    'Category': ['Linear', 'Ensemble', 'Non-Linear'],\n    'Best Model': ['LDA', 'Random Forest', 'KNN'],\n    'Accuracy': [0.7695, 0.8712, 0.8369],\n    'MCC': [0.6644, 0.8135, 0.7602],\n    'F1 (Weighted)': [0.7497, 0.8453, 0.8120]\n})\n\nprint(\"=\"*70)\nprint(\"BASELINE COMPARISON (Best from each category)\")\nprint(\"=\"*70)\nprint(baseline_results.to_string(index=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Optimised Model Comparison - VERIFIED DATA\noptimised_results = pd.DataFrame({\n    'Category': ['Linear', 'Ensemble', 'Non-Linear'],\n    'Model': ['LDA (optimised)', 'Random Forest (optimised)', 'KNN (optimised)'],\n    'Accuracy': [0.7748, 0.8709, 0.8752],\n    'MCC': [0.6712, 0.8146, 0.8161],\n    'F1 (Weighted)': [0.7628, 0.8401, 0.8655],\n    'Features': [30, 37, 30]\n})\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"OPTIMISED MODEL COMPARISON\")\nprint(\"=\"*70)\nprint(optimised_results.to_string(index=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# MCC Per Attack Class Comparison - VERIFIED DATA\nmcc_per_class = pd.DataFrame({\n    'Attack Class': ['benign', 'dos', 'probe', 'r2l', 'u2r'],\n    'LDA': [0.673, 0.786, 0.575, 0.513, 0.579],\n    'Random Forest': [0.763, 0.984, 0.926, 0.330, 0.820],\n    'KNN': [0.786, 0.946, 0.850, 0.567, 0.572]\n})\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"MCC PER ATTACK CLASS (Optimised Models)\")\nprint(\"=\"*70)\nprint(mcc_per_class.to_string(index=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualization: MCC Comparison - VERIFIED DATA\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Overall MCC - VERIFIED VALUES\nmodels = ['LDA', 'Random Forest', 'KNN']\nbaseline_mcc = [0.6644, 0.8135, 0.7602]  # Verified baseline MCC\noptimised_mcc = [0.6712, 0.8146, 0.8161]  # Verified optimised MCC\n\nx = np.arange(len(models))\nwidth = 0.35\n\nbars1 = axes[0].bar(x - width/2, baseline_mcc, width, label='Baseline', color='steelblue')\nbars2 = axes[0].bar(x + width/2, optimised_mcc, width, label='Optimised', color='darkorange')\naxes[0].set_ylabel('MCC Score')\naxes[0].set_title('Overall MCC: Baseline vs Optimised')\naxes[0].set_xticks(x)\naxes[0].set_xticklabels(models)\naxes[0].legend()\naxes[0].set_ylim(0, 1)\n\n# Add value labels on bars\nfor bar in bars1:\n    axes[0].annotate(f'{bar.get_height():.3f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n                     ha='center', va='bottom', fontsize=9)\nfor bar in bars2:\n    axes[0].annotate(f'{bar.get_height():.3f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n                     ha='center', va='bottom', fontsize=9)\n\n# MCC per class (optimised only) - VERIFIED VALUES\nattack_classes = ['benign', 'dos', 'probe', 'r2l', 'u2r']\nlda_mcc = [0.673, 0.786, 0.575, 0.513, 0.579]     # From verified results\nrf_mcc = [0.763, 0.984, 0.926, 0.330, 0.820]      # From verified results\nknn_mcc = [0.786, 0.946, 0.850, 0.567, 0.572]     # From verified results\n\nx2 = np.arange(len(attack_classes))\nwidth2 = 0.25\n\naxes[1].bar(x2 - width2, lda_mcc, width2, label='LDA', color='#2ecc71')\naxes[1].bar(x2, rf_mcc, width2, label='Random Forest', color='#3498db')\naxes[1].bar(x2 + width2, knn_mcc, width2, label='KNN', color='#e74c3c')\naxes[1].set_ylabel('MCC Score')\naxes[1].set_title('MCC per Attack Class (Optimised Models)')\naxes[1].set_xticks(x2)\naxes[1].set_xticklabels(attack_classes)\naxes[1].legend()\naxes[1].set_ylim(0, 1.1)\n\nplt.tight_layout()\nplt.savefig('../figures/all_classifiers_comparison.png', dpi=150)\nplt.show()\n\nprint(\"\\nIMPROVEMENT SUMMARY:\")\nprint(f\"LDA:           {baseline_mcc[0]:.4f} → {optimised_mcc[0]:.4f} (+{(optimised_mcc[0]-baseline_mcc[0])*100:.2f}%)\")\nprint(f\"Random Forest: {baseline_mcc[1]:.4f} → {optimised_mcc[1]:.4f} (+{(optimised_mcc[1]-baseline_mcc[1])*100:.2f}%)\")\nprint(f\"KNN:           {baseline_mcc[2]:.4f} → {optimised_mcc[2]:.4f} (+{(optimised_mcc[2]-baseline_mcc[2])*100:.2f}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. What Each Implementation Added Beyond Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Linear Classifier (LDA)\n",
    "\n",
    "| Feature | Template | Our Implementation |\n",
    "|---------|----------|--------------------|\n",
    "| Baselines compared | 1 (LDA only) | 3 (LDA, LogReg, Ridge) |\n",
    "| Hyperparameter tuning | None | 5-fold CV (solver, shrinkage) |\n",
    "| Feature selection | None | Correlation-based (122 → 30) |\n",
    "| MCC per class | Not shown | All 5 classes |\n",
    "| Bias-variance | Template | Full decomposition |\n",
    "| Visualizations | None | Confusion matrices, bar charts |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Ensemble Classifier (Random Forest)\n",
    "\n",
    "| Feature | Template | Our Implementation |\n",
    "|---------|----------|--------------------|\n",
    "| Baselines compared | 0 (commented out) | 3 (RF, ExtraTrees, AdaBoost) |\n",
    "| Hyperparameter tuning | None | RandomizedSearchCV (10 iter) |\n",
    "| Feature selection | None | Importance-based (122 → 38) |\n",
    "| Parameters tuned | None | 6 parameters |\n",
    "| Justification table | None | With references |\n",
    "| Feature importance plot | None | Top 30 features |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Non-Linear Classifier (KNN)\n",
    "\n",
    "| Feature | Template | Our Implementation |\n",
    "|---------|----------|--------------------|\n",
    "| Baselines compared | 0 (commented out) | 3 (KNN, DT, SVM) |\n",
    "| Hyperparameter tuning | None | GridSearchCV (16 combinations) |\n",
    "| Feature selection | None | Correlation-based (122 → 30) |\n",
    "| Parameters tuned | None | 4 parameters (k, weights, p, algorithm) |\n",
    "| Distance metrics | Default | Manhattan vs Euclidean compared |\n",
    "| Best result | N/A | **MCC 0.816** (highest overall) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Alignment with Assignment Requirements\n",
    "\n",
    "Based on professor's lecture (from transcript):\n",
    "\n",
    "| Requirement | Linear | Ensemble | Non-Linear |\n",
    "|-------------|--------|----------|------------|\n",
    "| Compare 3 baseline algorithms | ✅ | ✅ | ✅ |\n",
    "| Select best using MCC | ✅ LDA | ✅ RF | ✅ KNN |\n",
    "| Apply optimization technique | ✅ HP Tuning + Feature Sel | ✅ HP Tuning + Feature Sel | ✅ HP Tuning + Feature Sel |\n",
    "| Compare baseline vs optimised | ✅ | ✅ | ✅ |\n",
    "| Show MCC per attack class | ✅ | ✅ | ✅ |\n",
    "| Bias-variance decomposition | ✅ | ✅ | ✅ |\n",
    "| Hyperparameter justification table | ✅ | ✅ | ✅ |\n",
    "| Code is readable | ✅ | ✅ | ✅ |\n",
    "| Visualizations included | ✅ | ✅ | ✅ |\n",
    "| Results saved to files | ✅ JSON+CSV | ✅ JSON+CSV | ✅ JSON+CSV |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 6. Final Summary\n\n### Best Overall Model: **KNN (Optimised)**\n- **MCC: 0.8161** (highest)\n- **Accuracy: 87.52%**\n- **Features: 30** (75.4% reduction from 122)\n- **Member: Md Sohel Rana (TP087437)**\n\n### Performance Ranking (by MCC)\n\n| Rank | Model | MCC | Accuracy | Member |\n|------|-------|-----|----------|--------|\n| 1 | **KNN (optimised)** | **0.8161** | 87.52% | Md Sohel Rana (TP087437) |\n| 2 | Random Forest (optimised) | 0.8146 | 87.09% | Imran Shahadat Noble (TP087895) |\n| 3 | LDA (optimised) | 0.6712 | 77.48% | Muhammad Usama Fazal (TP086008) |\n\n### MCC Improvement from Baseline to Optimised\n\n| Model | Baseline MCC | Optimised MCC | Improvement |\n|-------|-------------|---------------|-------------|\n| LDA | 0.6644 | 0.6712 | +1.02% |\n| Random Forest | 0.8135 | 0.8146 | +0.14% |\n| **KNN** | **0.7602** | **0.8161** | **+7.35%** |\n\n### Key Findings\n\n1. **Non-linear models outperform linear** for this intrusion detection task\n   - KNN achieved highest MCC (0.8161) vs LDA (0.6712)\n\n2. **Feature reduction** improves KNN significantly (+7.35% MCC)\n   - Reduced from 122 to 30 features (75.4% reduction)\n\n3. **Distance metric matters for KNN**\n   - Manhattan distance (p=1) outperforms Euclidean for this dataset\n\n4. **Attack class performance varies**\n   - **DoS attacks**: Easiest to detect (RF: 0.984, KNN: 0.946)\n   - **R2L attacks**: Hardest to detect (RF: 0.330, KNN: 0.567, LDA: 0.513)\n\n5. **Ensemble methods excel at common attacks**\n   - Random Forest achieves 0.984 MCC on DoS, 0.926 on Probe\n\n### Optimization Strategies Results\n\n| Strategy | LDA Impact | RF Impact | KNN Impact |\n|----------|-----------|-----------|------------|\n| Hyperparameter Tuning | +0.68% | +0.14% | +5.59% |\n| Feature Selection | Correlation filter (30 features) | Importance (37 features) | Correlation filter (30 features) |\n| **Combined Effect** | **+1.02%** | **+0.14%** | **+7.35%** |\n\n### Conclusion\n\nThe **KNN classifier** with Manhattan distance (p=1), k=3 neighbours, and distance-weighted voting achieves the best overall performance. The significant improvement (+7.35% MCC) demonstrates that KNN benefits greatly from proper hyperparameter tuning and feature selection, particularly removing redundant/correlated features.\n\n**Data Verified:** 2024-12-13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Files Generated\n",
    "\n",
    "### Notebooks\n",
    "- `skML-complete.ipynb` - Professor's template\n",
    "- `01_Linear_Classifier.ipynb` - LDA implementation\n",
    "- `02_Ensemble_Classifier.ipynb` - Random Forest implementation\n",
    "- `03_NonLinear_Classifier.ipynb` - KNN implementation\n",
    "- `04_Group_Comparison.ipynb` - Group comparison\n",
    "\n",
    "### Results (JSON)\n",
    "- `results/linear_lda_results.json`\n",
    "- `results/ensemble_rf_results.json`\n",
    "- `results/nonlinear_knn_results.json`\n",
    "\n",
    "### Figures\n",
    "- `figures/linear_feature_correlation.png`\n",
    "- `figures/linear_baseline_vs_optimised.png`\n",
    "- `figures/linear_confusion_matrices.png`\n",
    "- `figures/ensemble_feature_importance.png`\n",
    "- `figures/ensemble_confusion_matrices.png`\n",
    "- `figures/nonlinear_feature_correlation.png`\n",
    "- `figures/nonlinear_confusion_matrices.png`\n",
    "\n",
    "---\n",
    "*Comprehensive comparison for DACS Assignment*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}