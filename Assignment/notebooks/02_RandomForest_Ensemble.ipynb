{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Assignment: Ensemble Classifier (Bagging)\n",
    "## Network Intrusion Detection using Random Forest\n",
    "\n",
    "**Classifier Category:** Ensemble (Bagging)  \n",
    "**Algorithm:** Random Forest Classifier  \n",
    "**Dataset:** NSL-KDD (Boosted Train + Preprocessed Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "data_path = '../datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import local library\n",
    "import sys\n",
    "if \"../..\" not in sys.path:\n",
    "    sys.path.insert(0, '../..')\n",
    "\n",
    "from mylib import show_labels_dist, show_metrics, bias_var_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Boosted Train and Preprocessed Test datasets\n",
    "data_file = os.path.join(data_path, 'NSL_boosted-2.csv')\n",
    "train_df = pd.read_csv(data_file)\n",
    "print('Train Dataset: {} rows, {} columns'.format(train_df.shape[0], train_df.shape[1]))\n",
    "\n",
    "data_file = os.path.join(data_path, 'NSL_ppTest.csv')\n",
    "test_df = pd.read_csv(data_file)\n",
    "print('Test Dataset: {} rows, {} columns'.format(test_df.shape[0], test_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print('Missing Values - Train Set:', train_df.isnull().sum().sum())\n",
    "print('Missing Values - Test Set:', test_df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets for consistent preprocessing\n",
    "combined_df = pd.concat([train_df, test_df])\n",
    "print('Combined Dataset: {} rows, {} columns'.format(combined_df.shape[0], combined_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check label distributions\n",
    "print(\"Label distribution:\")\n",
    "print(combined_df['label'].value_counts())\n",
    "print(\"\\nAttack category distribution:\")\n",
    "print(combined_df['atakcat'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set classification target (Two-class: normal vs attack)\n",
    "twoclass = True\n",
    "\n",
    "if twoclass:\n",
    "    labels_df = combined_df['label'].copy()\n",
    "    labels_df[labels_df != 'normal'] = 'attack'\n",
    "else:\n",
    "    labels_df = combined_df[['atakcat']].copy()\n",
    "    labels_df.rename(columns={'atakcat':'label'}, inplace=True)\n",
    "    labels_df = labels_df.squeeze('columns')\n",
    "\n",
    "# Drop target features\n",
    "combined_df.drop(['label'], axis=1, inplace=True)\n",
    "combined_df.drop(['atakcat'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding categorical features\n",
    "categori = combined_df.select_dtypes(include=['object']).columns\n",
    "category_cols = categori.tolist()\n",
    "features_df = pd.get_dummies(combined_df, columns=category_cols)\n",
    "print('Features after encoding: {} columns'.format(features_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numeric columns for scaling\n",
    "numeri = combined_df.select_dtypes(include=['float64','int64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore train/test split\n",
    "X_train = features_df.iloc[:len(train_df),:].copy()\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "X_test = features_df.iloc[len(train_df):,:].copy()\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "y_train = labels_df[:len(train_df)]\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "y_test = labels_df[len(train_df):]\n",
    "y_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "for i in numeri:\n",
    "    arr = np.array(X_train[i])\n",
    "    scale = MinMaxScaler().fit(arr.reshape(-1, 1))\n",
    "    X_train[i] = scale.transform(arr.reshape(len(arr),1))\n",
    "    \n",
    "    arr = np.array(X_test[i])\n",
    "    X_test[i] = scale.transform(arr.reshape(len(arr),1))\n",
    "\n",
    "print(\"Scaling completed using MinMaxScaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save original datasets\n",
    "X_train_original = X_train.copy()\n",
    "X_test_original = X_test.copy()\n",
    "y_train_original = y_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. BASELINE MODEL: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create baseline model with default parameters\n",
    "baseline_model = RandomForestClassifier(random_state=42)\n",
    "print(\"Baseline Model:\", baseline_model)\n",
    "print(\"\\nDefault Parameters:\")\n",
    "for k, v in baseline_model.get_params().items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show label distribution\n",
    "show_labels_dist(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate baseline model\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trs = time()\n",
    "baseline_model.fit(X_train, y_train)\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "tre = time() - trs\n",
    "\n",
    "print(f\"Training Time: {tre:.2f} seconds\\n\")\n",
    "show_metrics(y_test, y_pred_baseline, baseline_model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias-Variance Decomposition for baseline\n",
    "print(\"\\nBias-Variance Decomposition (Baseline):\")\n",
    "bias_var_metrics(X_train, X_test, y_train, y_test, RandomForestClassifier(random_state=42), folds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store baseline metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "baseline_metrics = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_baseline),\n",
    "    'precision': precision_score(y_test, y_pred_baseline, pos_label='attack'),\n",
    "    'recall': recall_score(y_test, y_pred_baseline, pos_label='attack'),\n",
    "    'f1': f1_score(y_test, y_pred_baseline, pos_label='attack'),\n",
    "    'mcc': matthews_corrcoef(y_test, y_pred_baseline)\n",
    "}\n",
    "print(\"Baseline Metrics:\", baseline_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. OPTIMISATION STRATEGY 1: Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "\n",
    "# Define parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "print(\"Parameter Grid for Random Forest:\")\n",
    "for k, v in param_grid.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use RandomizedSearchCV for efficiency\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "print(\"Running Randomized Search CV (this may take a few minutes)...\")\n",
    "trs = time()\n",
    "\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_random.fit(X_train, y_train)\n",
    "tre = time() - trs\n",
    "\n",
    "print(f\"\\nSearch Time: {tre:.2f} seconds\")\n",
    "print(f\"\\nBest Parameters: {rf_random.best_params_}\")\n",
    "print(f\"Best CV Score: {rf_random.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store best parameters\n",
    "best_params = rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. OPTIMISATION STRATEGY 2: Feature Importance Based Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from baseline model\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': baseline_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 Most Important Features:\")\n",
    "print(feature_importances.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importances\n",
    "plt.figure(figsize=(12, 10))\n",
    "top_n = 30\n",
    "top_features = feature_importances.head(top_n)\n",
    "sns.barplot(x='importance', y='feature', data=top_features, palette='viridis')\n",
    "plt.title(f'Top {top_n} Feature Importances - Random Forest')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features with importance > threshold\n",
    "# Use cumulative importance approach - select features that contribute to 95% of importance\n",
    "feature_importances['cumulative'] = feature_importances['importance'].cumsum()\n",
    "threshold_95 = feature_importances[feature_importances['cumulative'] <= 0.95]\n",
    "\n",
    "selected_features = threshold_95['feature'].tolist()\n",
    "# Add a few more features to ensure we capture key signals\n",
    "if len(selected_features) < 20:\n",
    "    selected_features = feature_importances.head(20)['feature'].tolist()\n",
    "\n",
    "print(f\"\\nSelected {len(selected_features)} features (95% cumulative importance)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reduced datasets\n",
    "X_train_reduced = X_train[selected_features]\n",
    "X_test_reduced = X_test[selected_features]\n",
    "print(f\"Reduced feature set: {X_train_reduced.shape[1]} features (from {X_train.shape[1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. OPTIMISATION STRATEGY 3: Handling Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "print(\"Class Distribution in Training Set:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nClass Ratio: {y_train.value_counts()['attack'] / y_train.value_counts()['normal']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use class_weight='balanced' to handle imbalance\n",
    "print(\"\\nUsing class_weight='balanced' to handle class imbalance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. OPTIMISED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimised model with best parameters, selected features, and balanced weights\n",
    "optimised_params = best_params.copy()\n",
    "optimised_params['class_weight'] = 'balanced'\n",
    "optimised_params['random_state'] = 42\n",
    "\n",
    "optimised_model = RandomForestClassifier(**optimised_params)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"OPTIMISED MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Parameters: {optimised_params}\")\n",
    "print(f\"Features: {len(selected_features)} (reduced from {X_train.shape[1]})\")\n",
    "\n",
    "trs = time()\n",
    "optimised_model.fit(X_train_reduced, y_train)\n",
    "y_pred_optimised = optimised_model.predict(X_test_reduced)\n",
    "tre = time() - trs\n",
    "\n",
    "print(f\"\\nTraining Time: {tre:.2f} seconds\\n\")\n",
    "show_metrics(y_test, y_pred_optimised, optimised_model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias-Variance Decomposition for optimised model\n",
    "print(\"\\nBias-Variance Decomposition (Optimised):\")\n",
    "opt_model_for_bv = RandomForestClassifier(**optimised_params)\n",
    "bias_var_metrics(X_train_reduced, X_test_reduced, y_train, y_test, opt_model_for_bv, folds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store optimised metrics\n",
    "optimised_metrics = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_optimised),\n",
    "    'precision': precision_score(y_test, y_pred_optimised, pos_label='attack'),\n",
    "    'recall': recall_score(y_test, y_pred_optimised, pos_label='attack'),\n",
    "    'f1': f1_score(y_test, y_pred_optimised, pos_label='attack'),\n",
    "    'mcc': matthews_corrcoef(y_test, y_pred_optimised)\n",
    "}\n",
    "print(\"Optimised Metrics:\", optimised_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. COMPARISON: Baseline vs Optimised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'MCC'],\n",
    "    'Baseline': [baseline_metrics['accuracy'], baseline_metrics['precision'], \n",
    "                 baseline_metrics['recall'], baseline_metrics['f1'], baseline_metrics['mcc']],\n",
    "    'Optimised': [optimised_metrics['accuracy'], optimised_metrics['precision'],\n",
    "                  optimised_metrics['recall'], optimised_metrics['f1'], optimised_metrics['mcc']]\n",
    "})\n",
    "comparison_df['Improvement'] = comparison_df['Optimised'] - comparison_df['Baseline']\n",
    "comparison_df['Improvement %'] = (comparison_df['Improvement'] / comparison_df['Baseline'] * 100).round(2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE COMPARISON: BASELINE vs OPTIMISED\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(comparison_df['Metric']))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, comparison_df['Baseline'], width, label='Baseline', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, comparison_df['Optimised'], width, label='Optimised', color='forestgreen')\n",
    "\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Random Forest: Baseline vs Optimised')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Metric'])\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=8)\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Comparison\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "cm_baseline = confusion_matrix(y_test, y_pred_baseline, labels=baseline_model.classes_)\n",
    "disp1 = ConfusionMatrixDisplay(confusion_matrix=cm_baseline, display_labels=baseline_model.classes_)\n",
    "disp1.plot(ax=axes[0], cmap='Blues')\n",
    "axes[0].set_title('Baseline Model')\n",
    "\n",
    "cm_optimised = confusion_matrix(y_test, y_pred_optimised, labels=optimised_model.classes_)\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm_optimised, display_labels=optimised_model.classes_)\n",
    "disp2.plot(ax=axes[1], cmap='Greens')\n",
    "axes[1].set_title('Optimised Model')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve Comparison\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_prob_baseline = baseline_model.predict_proba(X_test)[:, 1]\n",
    "y_prob_optimised = optimised_model.predict_proba(X_test_reduced)[:, 1]\n",
    "\n",
    "y_test_binary = (y_test == 'attack').astype(int)\n",
    "\n",
    "fpr_base, tpr_base, _ = roc_curve(y_test_binary, y_prob_baseline)\n",
    "fpr_opt, tpr_opt, _ = roc_curve(y_test_binary, y_prob_optimised)\n",
    "\n",
    "auc_base = auc(fpr_base, tpr_base)\n",
    "auc_opt = auc(fpr_opt, tpr_opt)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_base, tpr_base, 'b-', label=f'Baseline (AUC = {auc_base:.4f})')\n",
    "plt.plot(fpr_opt, tpr_opt, 'g-', label=f'Optimised (AUC = {auc_opt:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison - Random Forest')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SUMMARY: RANDOM FOREST FOR INTRUSION DETECTION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n1. CLASSIFIER CATEGORY: Ensemble (Bagging)\")\n",
    "print(\"   Algorithm: Random Forest Classifier\")\n",
    "print(\"\\n2. OPTIMISATION STRATEGIES APPLIED:\")\n",
    "print(\"   a) Hyperparameter Tuning with RandomizedSearchCV\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"      - {k}: {v}\")\n",
    "print(\"   b) Feature Selection based on Feature Importance\")\n",
    "print(f\"      - Original features: {X_train.shape[1]}\")\n",
    "print(f\"      - Selected features: {len(selected_features)}\")\n",
    "print(f\"      - Feature reduction: {((X_train.shape[1] - len(selected_features)) / X_train.shape[1] * 100):.1f}%\")\n",
    "print(\"   c) Class Imbalance Handling\")\n",
    "print(\"      - Method: class_weight='balanced'\")\n",
    "print(\"\\n3. PERFORMANCE IMPROVEMENT:\")\n",
    "for _, row in comparison_df.iterrows():\n",
    "    print(f\"   {row['Metric']}: {row['Baseline']:.4f} -> {row['Optimised']:.4f} ({row['Improvement %']:+.2f}%)\")\n",
    "print(f\"\\n4. ROC-AUC: {auc_base:.4f} -> {auc_opt:.4f}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for group comparison\n",
    "import json\n",
    "\n",
    "results_dict = {\n",
    "    'classifier': 'Random Forest',\n",
    "    'category': 'Ensemble (Bagging)',\n",
    "    'baseline_metrics': baseline_metrics,\n",
    "    'optimised_metrics': optimised_metrics,\n",
    "    'baseline_auc': auc_base,\n",
    "    'optimised_auc': auc_opt,\n",
    "    'optimisation_strategies': ['Hyperparameter Tuning', 'Feature Selection (Importance)', 'Class Weighting'],\n",
    "    'best_params': {k: str(v) for k, v in best_params.items()},\n",
    "    'n_features_original': X_train.shape[1],\n",
    "    'n_features_selected': len(selected_features)\n",
    "}\n",
    "\n",
    "with open('../results/ensemble_rf_results.json', 'w') as f:\n",
    "    json.dump(results_dict, f, indent=2)\n",
    "print(\"Results saved to: results/ensemble_rf_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}