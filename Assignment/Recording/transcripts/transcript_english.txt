to you guys?
Ok, and stop sharing.
Alright, they don't record.
Okay, so this is the first one is the cover page, right?
The cover page will be similar, right?
With everyone, you write your group name and your DP number, right?
Alright, so it's the same, no issue.
The first one of course, the...
Okay, of course you have a table of 10, right?
As you can see, chapter 1 is always an introduction, I've explained to everyone already.
It's basically a brief review of what machine learning model each of your group members has selected.
Right? There are three of you, right?
There are three of you. In this case, I think there are three of them.
So they have selected a linear model, they have selected a support vector machine and also a linear model.
Just provide a brief overview, right?
And you can see that it's only... no, no, no, it's 2 to 6.
2 to 6 page, right? It's about 5.
So which is kind of reasonable for chapter 1, right?
In chapter 2, they will do a comparison of the minest model.
Because chapter 1 and chapter 2 are being marked as group work, group work, right?
The group work.
So how do they compare?
They are using their comparative accuracy in terms of bias and variance.
And they are using the cross validation method approach number 1.
To compare the optimized model from each of the team members, right?
And you can see nothing much is written also about 3, 4 pages and that's it, right?
Now come to the individual part, right?
Chapter 3.
Individual part basically... as I say, right?
You got to label your name and your team number so that I know it's you, right?
Because each of the group, only the group leader is to submit the PDF report only.
But when I mark, of course, I will distribute the mark to your own respective.
That's why I will actually do the grouping according to what you have submitted to me.
So that I will then distribute the marks.
So each of you will have your own marks.
But mark based on one report.
So linear model, you can see, right?
You can choose to have your own subsection, it's fine.
Because each of you are writing your own chapter, right?
So linear model, let's say this student and this team member.
Of course, the first thing you got to do is a comparison of baseline, right?
Let's say you have chosen a nonlinear model.
There are three algorithms, right?
So you need to actually summarize how their performance of the three algorithm from there you select one best baseline.
So you need to go through that process, right?
And then you need to talk about, of course, you can compare also the bias and variance value.
And for this group of student, right, they can only choose one optimization method.
But we have made some changes to the assignment.
We allow students to be flexible to choose more than one, right?
But for this group, they can only choose one.
So for her, she will choose feature selection using correlation, right?
The feature filter function, right?
And then she will explain how she do it and things like that.
Then you just need to compare between baseline or demise model, right?
Compare between baseline or demise model and then just provide your analysis.
So second student also the same comparison of baseline model.
Again, you know, they try to run through the accuracy comparison, bias, variance, comparison
and which particular baseline will be selected for optimization, right?
And then the optimization technique she's using is permutation feature importance.
And the third and also then compare between baseline and optimized model.
Basically, it's like that.
That's all you need to do for your own chapter, right?
Which baseline model is the best?
What optimization technique you have used to further optimize the baseline model?
And compare now the optimized model and the baseline model.
How's the performance improvement have been?
And that's all you need to do, right?
For your own chapter.
And the third student also the same, yeah?
The third student, she's using hyperparameter tuning, right?
So things like that.
Then there's it done, right?
Then a list of references.
That is the main body of report.
Only three chapters, right?
Then subsequently what you need is basically the respective appendix of your own
JupyterLab notebook scripts.
You do not need to submit a group-based comparison script there.
You just need to submit your own script starting from baseline all the way to optimized model.
And then you just put all the reservoir visualization.
Later I can show you.
Okay, then that's it, right?
And then you should have your own list of tables.
And then you have your own list of...
Where's the list of figures?
List of figures, then list of tables.
And keep it separated.
Don't put them together, yeah?
List of tables, one page.
Then list of tables, one one page.
Okay, so subsequently these are basically just
an introduction chapter, right?
So they actually provide a proof-of-all view of each of the group members.
What they use.
These will always be nice, right?
So that we know that, hey, this group is using all these three categories.
So they provide that.
And then they try to explain.
The new model, right?
Things like that.
This is entirely up to you to see how do you want to explain them, yeah?
The concise should be fine.
So they provide a comparison between all the linear model, right?
Then support vector model, so they explain.
You can see it's actually, you can make some references, some citation somewhere, right?
When you explain, you may need to look for some literature.
You can do that.
And then they actually compare between the three support vector machine models that selected.
A quick comparison.
Then nonlinear model, again, the comparison, right?
Some explanation.
And that's it.
Done.
All right?
Done for chapter one.
Chapter two, they compare the optimized comparison of optimized model, right?
So you can see, right, they have this compare.
How do they compare?
They can compare in terms of accuracy, right?
You can see that you can compare that and the MCC of different, because you are asked to detect different class, right?
Deny, deny a service, propane, remove to local and user to root, right?
So you have your MCC overall and also you have the respective MCC score for each of the category, right?
So they just need to compare between the three optimized model, because, right, the optimized model are decision three, support vector, and also logistic regression from each of the group.
So they just do a simple comparison of MCC.
And after that, write some explanation, right?
Some interpretation.
And this is important, right?
Because this master level, you just do not just put a figure and say nothing.
You need to perform some critical analysis, right?
A comparison to actually try to understand what does this telling us, right?
And then compare the accuracy score with the optimized model.
Of course, you can see here, decision three is the best so far, right?
Then they actually write some explanation, right?
Then after that, they also compare the bias and variance, right?
Of course, they forgot to put a reference into the table or every table they mentioned.
So, but they do a comparison of bias and variance, right?
You can see, right?
And then again, write some comparison.
And with that, they actually make some conclusion which model actually performed best in terms of bias and variance, right?
Decision three logistic regression of whatsoever.
Then also, now they are using the cross-validation technique, right?
So what you need to do is very simple.
You just need to perform the cross-validation.
They are using the bias-variant-based cross-validation, the one that we look at.
There are two, right?
One is, I think, not sure, I think they should have the accuracy base as well.
You just need to plot polylapse in a simple biograph, right?
After you run the cross-validation of how many folds, what is the average bias value, right?
What is the average variance value and what is the average expected loss?
Of course, overall decision three still performed better because the expected loss of decision three is the lowest, right?
So this is also a cross-validation presentation based on the balance accuracy, right?
This is another one.
Again, accuracy, you look at the higher the better, so decision three also wins.
You can see that it's actually very simple, right?
How do you compare with your team members' optimized model?
Just if you want, you can run the cross-validation, get your own figure for bias and variance and also for balance accuracy.
And then everyone just grouped together the data.
You want to present it in biograph or you want to present it in table and then try to explain.
And then finally, the conclusion is what?
Decision three performed the best, right?
And try to actually make a conclusion out of the three of the demands model, which model is the best, right?
And then they have included this comparison.
Overall, decision three model is the best choice because it has the highest balance accuracy and the lowest error and loss.
Making it the most effective model for the attack classification task, something like that.
And that's it, the comparison chapter done.
Again, not a lot of things to be written, isn't it?
You can see, right, all these graphs and figures are not produced by Jupyter Lab.
That's why any graph that you produce by Jupyter Lab keep it within the script.
Don't copy and paste it inside the main report.
The main report basically, mini table and some bar graph, something like that for comparison, right?
Simple one will do.
Okay, so individual chapter, again you can see, right?
Individual chapter, similar thing.
They run the comparison. This is for by one student.
They run the comparison of baseline model, right?
She has selected a linear model, so she has four baselines, right?
So you just run an overall comparison of the MCC, for example, for all the baseline model, right?
And then you also run a comparison of bias and variance for all the baseline model.
From there, you make your conclusion of what is the best baseline.
So baseline, that should make conclusion here, right?
Logistic regression has the highest accuracy from the models.
It has also the highest accuracy at the overall MCC.
And that's why logistic regression will be the best baseline that she has selected.
And then after that, she will then further perform optimization by using feature selection using correlation, right?
You can explain a little bit what do you mean by feature selection using correlation?
You provide some explanation, right?
What does it do, right?
Then after that, she just presents, or after she optimizes, she do get better after optimization, right?
In this case, she do not get better accuracy.
You can see, right? She do not get better.
Then you do not get better, then she tries to explain.
But now, because we allow students to use more than one optimization technique, right?
I will expect you to get better performance.
You should, if you have investigated enough, right?
You are able to further improve the performance of your chosen machine learning model.
In no way, you can say that you cannot get improved performance.
Unless you did not do, I think, yeah, our last-minute job.
If you, yeah, I'm serious.
If you are really looking at your own algorithm, you have to have hyperparameter training for sure,
plus a feature-based technique, right?
I'm sure you will get better performance.
Even a slight percentage increase, it's okay, right?
At least you perform better than the baseline.
Okay, this is a program machine.
Again, some comparison.
Different students have different way of presenting.
You use your own way.
It's okay, yeah? You use your own way.
I want to show you is the hyperparameter tuning.
She says she's using permutation feature importance,
and then she explains what is permutation feature importance,
and then explains a little bit the process,
and then just do a comparison between support vector and also the optimized one, yeah?
What I want to show you is this one, the hyperparameter tuning.
For the hyperparameter tuning, right?
You see, right? She says she's using research cross-parameter.
It's important for you to have some kind of table,
because you got to let the marker, which is myself, know
what kind of hyperparameter you have selected for this algorithm, right?
This is the decision-trait.
These are all the hyperparameter for decision-trait, right?
At least you should provide some recommendation.
These are the hyperparameters, and you must actually do some research on
what is the kind of range of value you should set for each parameter.
You cannot say that I just try an error, I just put 100, 250, and see what happens, right?
Each of the parameters will have a set of recommended value, right?
And also, you actually can provide some referencing to some of the important literature.
You can explain what does each of these hyperparameter mean, right?
And because you only focus on one machine learning algorithm, you are able to do that.
You are not asking you to look at everything, right?
Let's say you only look at the decision-trait, or you only look at oxygen regression,
or you go to furthering on that algorithm itself.
Once you have selected a best baseline, after that, you should have this table
to explain what is the value that you have set to tune.
Like here, the max depth, by default, there's no limitation, right?
So she has selected none, 20 or 30 to tune.
And then what is the reason why this value is being selected?
Alright, so...
Alright, why this value is being selected?
So, down.
Right? Let's say a max feature.
Why the value is being selected, right?
So you must have this table to provide your justification
on why to select those values for this particular hyperparameter.
If you can support yourself with some references, it's okay.
Sometimes you can refer to an article, right?
They will let you know that a typically max depth should be set to this range of values.
There is some reason to support you, right?
In choosing your hyperparameter for the tuning process, right?
And after that, you just do a normal thing, comparison,
between two mines and baseline model, I think so, that is normal, right?
And last but not least, we have your references in the APA format.
And the rest are just scripts.
And one thing that I want to mention is that your appendix, right?
You've got to write your name and keep it number.
And please make sure that the code are readable.
I can show you samples of code which is totally not readable.
Because when you submit a PDF, right?
There's a 30% of marks given to your code, you know?
And if I cannot read it, it's how, how do I, or why marks, right?
So, what I draw my best was so difficult for me.
Like the code need to be readable, of course this projector is kind of bad.
But it's quite clear actually, you can read very clearly.
And some of the codes of course is standard, right?
Isn't it? Because the script files are given.
But you started to insert your own, I think somewhere down the line, just a while here.
These are still standard.
So, you have selected the linear model, right?
Then, and then you have your baseline comparison as per normal.
So, you present them on that.
And these are all warnings.
Yeah, they're not error.
Then you try to have some visualization, right?
Visualization.
And then you compare the bias and variance.
And now this is the baseline model.
This is where you started to have your own code.
So, the best baseline is logistic regression.
So, she used a feature, feature filter, filter SCY.
And then the rest is basically similar.
Keep list, you know, and all this.
Then hyper parameter tuning.
Of course, she did not do that.
And then she actually performed some visualization.
You can see here, visualization.
And the confusion metrics, plotting, I think, later.
So, there are some forms of visualization there.
Right?
And the second, I think this student will have a lot more.
And you can see, right, you can basically try to do your own
segmentation, explanation, just a while here.
Some graphics visualization.
Right?
And then confusion metrics.
Right?
After the trick method, you can plot confusion metrics.
Right?
Hit map.
And things like that.
Basically, all the visualization techniques I have shared in the script file, right?
You can just copy and paste and modify the value.
The earlier that you do it, right, the better you know whatever error you've got.
At least we can discuss it and show me.
Right?
So, it depends on what optimization method you use.
We have one server script file for each of the methods.
You can customize it and put it inside the code.
Right?
So, these are all the different script files.
You have different visualization.
And this one, at least I can still zoom in.
I can still read.
So, you really have to ensure that when you copy and paste things,
you will re-put a good resolution one.
Not just a very brilliant one.
Or else, it really can only...
And please insert visualization.
Because if you deny visualization, I think automatically you will lose...
This is more because they are using hyper-parameter tuning, right?
So, they need to look at a lot of different parameters.
Let me show you the code.
Very nice tuning done.
You can see because at one time,
these are the values being looked at.
Because it forms a grid, right?
They try to actually run through this value
to see which combination of values is the best.
So, if you wanted to use hyper-parameter tuning,
I would suggest you to look at a minimum of three hyper-parameters.
Don't tell me you are only using two.
Of course, the more the better.
But also be realistic because you will actually occupy the computation time.
It depends on your computation power as well.
Sometimes it will take a long time to run through.
You just imagine, right?
Let's say you have so many, right?
You have so many.
Everything multiplied together.
Two times two times two times two times two times two times two times two.
And then with five-fold only,
you may have easily how many runs in this case.