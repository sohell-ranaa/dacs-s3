# DACS Assignment Briefing - Corrected Transcript
# Professor explaining sample assignment structure
# Corrections based on ML/Cybersecurity context

---

[INTRO]
How about you guys?
Ok, and stop sharing.
Alright, they don't record.

[COVER PAGE]
Okay, so this is the first one - the cover page, right?
The cover page will be similar, right?
With everyone, you write your group name and your TP number, right?
Alright, so it's the same, no issue.

[TABLE OF CONTENTS]
The first one of course...
Okay, of course you have a table of contents, right?
As you can see, Chapter 1 is always an introduction, I've explained to everyone already.
It's basically a brief review of what machine learning model each of your group members has selected.
Right? There are three of you, right?
There are three of you. In this case, I think there are three of them.
So they have selected a linear model, they have selected a Support Vector Machine, and also a nonlinear model.
Just provide a brief overview, right?
And you can see that it's only... no, no, no, it's 2 to 6.
2 to 6 pages, right? It's about 5.
So which is kind of reasonable for Chapter 1, right?

[CHAPTER 2 - GROUP COMPARISON]
In Chapter 2, they will do a comparison of the optimized models.
Because Chapter 1 and Chapter 2 are being marked as group work, group work, right?
The group work.
So how do they compare?
They are using their comparative accuracy in terms of bias and variance.
And they are using the cross-validation method approach number 1.
To compare the optimized model from each of the team members, right?
And you can see nothing much is written - also about 3, 4 pages and that's it, right?

[CHAPTER 3 - INDIVIDUAL PART]
Now come to the individual part, right?
Chapter 3.
Individual part basically... as I say, right?
You got to label your name and your TP number so that I know it's you, right?
Because each of the group, only the group leader is to submit the PDF report only.
But when I mark, of course, I will distribute the mark to your own respective.
That's why I will actually do the grouping according to what you have submitted to me.
So that I will then distribute the marks.
So each of you will have your own marks.
But mark based on one report.

[LINEAR MODEL STUDENT]
So linear model, you can see, right?
You can choose to have your own subsection, it's fine.
Because each of you are writing your own chapter, right?
So linear model, let's say this student and this TP number.
Of course, the first thing you got to do is a comparison of baseline, right?
Let's say you have chosen a nonlinear model.
There are three algorithms, right?
So you need to actually summarize how their performance of the three algorithms - from there you select one best baseline.
So you need to go through that process, right?
And then you need to talk about, of course, you can compare also the bias and variance value.

[OPTIMIZATION METHODS]
And for this group of students, right, they can only choose one optimization method.
But we have made some changes to the assignment.
We allow students to be flexible to choose more than one, right?
But for this group, they can only choose one.
So for her, she will choose feature selection using correlation, right?
The feature filter function, right?
And then she will explain how she does it and things like that.
Then you just need to compare between baseline and optimized model, right?
Compare between baseline and optimized model and then just provide your analysis.

[SECOND STUDENT]
So second student also the same - comparison of baseline model.
Again, you know, they try to run through the accuracy comparison, bias variance comparison,
and which particular baseline will be selected for optimization, right?
And then the optimization technique she's using is permutation feature importance.
And the third - and also then compare between baseline and optimized model.
Basically, it's like that.

[WHAT TO INCLUDE]
That's all you need to do for your own chapter, right?
Which baseline model is the best?
What optimization technique you have used to further optimize the baseline model?
And compare now the optimized model and the baseline model.
How's the performance improvement have been?
And that's all you need to do, right?
For your own chapter.
And the third student also the same, yeah?
The third student, she's using hyperparameter tuning, right?
So things like that.
Then there's it done, right?

[REFERENCES AND APPENDIX]
Then a list of references.
That is the main body of report.
Only three chapters, right?
Then subsequently what you need is basically the respective appendix of your own JupyterLab notebook scripts.
You do not need to submit a group-based comparison script there.
You just need to submit your own script starting from baseline all the way to optimized model.
And then you just put all the result visualization.
Later I can show you.
Okay, then that's it, right?

[LIST OF TABLES AND FIGURES]
And then you should have your own list of tables.
And then you have your own list of...
Where's the list of figures?
List of figures, then list of tables.
And keep it separated.
Don't put them together, yeah?
List of tables, one page.
Then list of figures, one page.

[CHAPTER 1 DETAILS]
Okay, so subsequently these are basically just an introduction chapter, right?
So they actually provide a preferable view of each of the group members.
What they use.
These will always be nice, right?
So that we know that, hey, this group is using all these three categories.
So they provide that.
And then they try to explain.
Linear model, right?
Things like that.
This is entirely up to you to see how do you want to explain them, yeah?
Concise should be fine.
So they provide a comparison between all the linear models, right?
Then Support Vector Machine model, so they explain.
You can see it's actually, you can make some references, some citation somewhere, right?
When you explain, you may need to look for some literature.
You can do that.
And then they actually compare between the three Support Vector Machine models that selected.
A quick comparison.
Then nonlinear model, again, the comparison, right?
Some explanation.
And that's it.
Done.
All right?
Done for Chapter 1.

[CHAPTER 2 - OPTIMIZED MODEL COMPARISON]
Chapter 2, they compare the optimized models, right?
So you can see, right, they have this comparison.
How do they compare?
They can compare in terms of accuracy, right?
You can see that you can compare that and the MCC of different classes, because you are asked to detect different attack classes, right?
DoS (Denial of Service), Probe, Remote to Local (R2L), and User to Root (U2R), right?
So you have your MCC overall and also you have the respective MCC score for each of the category, right?
So they just need to compare between the three optimized models, because, right, the optimized models are Decision Tree, Support Vector Machine, and also Logistic Regression from each of the group.
So they just do a simple comparison of MCC.
And after that, write some explanation, right?
Some interpretation.
And this is important, right?
Because this is master level, you just do not just put a figure and say nothing.
You need to perform some critical analysis, right?
A comparison to actually try to understand what does this telling us, right?
And then compare the accuracy score with the optimized model.
Of course, you can see here, Decision Tree is the best so far, right?
Then they actually write some explanation, right?

[BIAS VARIANCE COMPARISON]
Then after that, they also compare the bias and variance, right?
Of course, they forgot to put a reference into the table or every table they mentioned.
So, but they do a comparison of bias and variance, right?
You can see, right?
And then again, write some comparison.
And with that, they actually make some conclusion which model actually performed best in terms of bias and variance, right?
Decision Tree, Logistic Regression, or whatsoever.

[CROSS-VALIDATION]
Then also, now they are using the cross-validation technique, right?
So what you need to do is very simple.
You just need to perform the cross-validation.
They are using the bias-variance-based cross-validation, the one that we looked at.
There are two, right?
One is, I think, not sure, I think they should have the accuracy-based as well.
You just need to plot it up in a simple bar graph, right?
After you run the cross-validation of how many folds, what is the average bias value, right?
What is the average variance value and what is the average expected loss?
Of course, overall Decision Tree still performed better because the expected loss of Decision Tree is the lowest, right?
So this is also a cross-validation presentation based on the balanced accuracy, right?
This is another one.
Again, accuracy, you look at the higher the better, so Decision Tree also wins.
You can see that it's actually very simple, right?

[TEAM COMPARISON]
How do you compare with your team members' optimized model?
Just if you want, you can run the cross-validation, get your own figure for bias and variance and also for balanced accuracy.
And then everyone just grouped together the data.
You want to present it in bar graph or you want to present it in table and then try to explain.
And then finally, the conclusion is what?
Decision Tree performed the best, right?
And try to actually make a conclusion out of the three of the team's models, which model is the best, right?
And then they have included this comparison.
Overall, Decision Tree model is the best choice because it has the highest balanced accuracy and the lowest error and loss.
Making it the most effective model for the attack classification task, something like that.
And that's it, the comparison chapter done.

[GRAPH GUIDELINES]
Again, not a lot of things to be written, isn't it?
You can see, right, all these graphs and figures are not produced by JupyterLab.
That's why any graph that you produce by JupyterLab keep it within the script.
Don't copy and paste it inside the main report.
The main report basically, mini table and some bar graph, something like that for comparison, right?
Simple one will do.

[INDIVIDUAL CHAPTER EXAMPLE]
Okay, so individual chapter, again you can see, right?
Individual chapter, similar thing.
They run the comparison. This is for by one student.
They run the comparison of baseline model, right?
She has selected a linear model, so she has four baselines, right?
So you just run an overall comparison of the MCC, for example, for all the baseline models, right?
And then you also run a comparison of bias and variance for all the baseline models.
From there, you make your conclusion of what is the best baseline.
So baseline, that should make conclusion here, right?
Logistic Regression has the highest accuracy from the models.
It has also the highest accuracy at the overall MCC.
And that's why Logistic Regression will be the best baseline that she has selected.

[FEATURE SELECTION]
And then after that, she will then further perform optimization by using feature selection using correlation, right?
You can explain a little bit what do you mean by feature selection using correlation?
You provide some explanation, right?
What does it do, right?
Then after that, she just presents, or after she optimizes, she do get better after optimization, right?
In this case, she do not get better accuracy.
You can see, right? She do not get better.
Then you do not get better, then she tries to explain.

[EXPECTATION FOR IMPROVEMENT]
But now, because we allow students to use more than one optimization technique, right?
I will expect you to get better performance.
You should, if you have investigated enough, right?
You are able to further improve the performance of your chosen machine learning model.
In no way, you can say that you cannot get improved performance.
Unless you did not do, I think, yeah, our last-minute job.
If you, yeah, I'm serious.
If you are really looking at your own algorithm, you have to have hyperparameter tuning for sure,
plus a feature-based technique, right?
I'm sure you will get better performance.
Even a slight percentage increase, it's okay, right?
At least you perform better than the baseline.

[SUPPORT VECTOR MACHINE EXAMPLE]
Okay, this is Support Vector Machine.
Again, some comparison.
Different students have different way of presenting.
You use your own way.
It's okay, yeah? You use your own way.

[HYPERPARAMETER TUNING]
I want to show you is the hyperparameter tuning.
She says she's using permutation feature importance,
and then she explains what is permutation feature importance,
and then explains a little bit the process,
and then just do a comparison between Support Vector Machine and also the optimized one, yeah?
What I want to show you is this one, the hyperparameter tuning.
For the hyperparameter tuning, right?
You see, right? She says she's using Randomized Search Cross-Validation.
It's important for you to have some kind of table,
because you got to let the marker, which is myself, know
what kind of hyperparameter you have selected for this algorithm, right?
This is the Decision Tree.
These are all the hyperparameters for Decision Tree, right?
At least you should provide some recommendation.
These are the hyperparameters, and you must actually do some research on
what is the kind of range of value you should set for each parameter.
You cannot say that I just try and error, I just put 100, 250, and see what happens, right?
Each of the parameters will have a set of recommended values, right?
And also, you actually can provide some referencing to some of the important literature.
You can explain what does each of these hyperparameters mean, right?
And because you only focus on one machine learning algorithm, you are able to do that.
You are not asking you to look at everything, right?
Let's say you only look at the Decision Tree, or you only look at Logistic Regression,
or you go further in on that algorithm itself.

[HYPERPARAMETER TABLE]
Once you have selected a best baseline, after that, you should have this table
to explain what is the value that you have set to tune.
Like here, the max_depth, by default, there's no limitation, right?
So she has selected None, 20 or 30 to tune.
And then what is the reason why this value is being selected?
Alright, so...
Alright, why this value is being selected?
So, down.
Right? Let's say max_features.
Why the value is being selected, right?
So you must have this table to provide your justification
on why to select those values for this particular hyperparameter.
If you can support yourself with some references, it's okay.
Sometimes you can refer to an article, right?
They will let you know that a typically max_depth should be set to this range of values.
There is some reason to support you, right?
In choosing your hyperparameter for the tuning process, right?

[FINAL COMPARISON]
And after that, you just do a normal thing, comparison,
between optimized and baseline model, I think so, that is normal, right?
And last but not least, we have your references in the APA format.
And the rest are just scripts.

[APPENDIX CODE]
And one thing that I want to mention is that your appendix, right?
You've got to write your name and TP number.
And please make sure that the code are readable.
I can show you samples of code which is totally not readable.
Because when you submit a PDF, right?
There's a 30% of marks given to your code, you know?
And if I cannot read it, how do I give marks, right?
So, what I draw my best was so difficult for me.
Like the code need to be readable, of course this projector is kind of bad.
But it's quite clear actually, you can read very clearly.
And some of the codes of course is standard, right?
Isn't it? Because the script files are given.
But you started to insert your own, I think somewhere down the line, just a while here.
These are still standard.
So, you have selected the linear model, right?
Then, and then you have your baseline comparison as per normal.
So, you present them on that.
And these are all warnings.
Yeah, they're not errors.

[VISUALIZATION]
Then you try to have some visualization, right?
Visualization.
And then you compare the bias and variance.
And now this is the baseline model.
This is where you started to have your own code.
So, the best baseline is Logistic Regression.
So, she used a feature filter, SelectKBest.
And then the rest is basically similar.
K-fold list, you know, and all this.
Then hyperparameter tuning.
Of course, she did not do that.
And then she actually performed some visualization.
You can see here, visualization.
And the confusion matrix, plotting, I think, later.
So, there are some forms of visualization there.
Right?
And the second, I think this student will have a lot more.
And you can see, right, you can basically try to do your own
segmentation, explanation, just a while here.
Some graphics visualization.
Right?
And then confusion matrix.
Right?
After the predict method, you can plot confusion matrix.
Right?
Heatmap.
And things like that.
Basically, all the visualization techniques I have shared in the script file, right?
You can just copy and paste and modify the value.
The earlier that you do it, right, the better you know whatever error you've got.
At least we can discuss it and show me.
Right?
So, it depends on what optimization method you use.
We have one server script file for each of the methods.
You can customize it and put it inside the code.
Right?
So, these are all the different script files.
You have different visualization.

[CODE QUALITY]
And this one, at least I can still zoom in.
I can still read.
So, you really have to ensure that when you copy and paste things,
you will put a good resolution one.
Not just a very blurry one.
Or else, it really can only...
And please insert visualization.
Because if you deny visualization, I think automatically you will lose marks.

[GRID SEARCH]
This is more because they are using hyperparameter tuning, right?
So, they need to look at a lot of different parameters.
Let me show you the code.
Very nice tuning done.
You can see because at one time,
these are the values being looked at.
Because it forms a grid, right?
They try to actually run through this value
to see which combination of values is the best.
So, if you wanted to use hyperparameter tuning,
I would suggest you to look at a minimum of three hyperparameters.
Don't tell me you are only using two.
Of course, the more the better.
But also be realistic because you will actually occupy the computation time.
It depends on your computation power as well.
Sometimes it will take a long time to run through.
You just imagine, right?
Let's say you have so many, right?
You have so many.
Everything multiplied together.
Two times two times two times two times two times two times two times two.
And then with five-fold cross-validation,
you may have easily how many runs in this case.

---
# KEY CORRECTIONS MADE:
# - "DP number" → "TP number" (student ID)
# - "decision three" → "Decision Tree" (ML algorithm)
# - "propane" → "Probe" (attack class)
# - "remove to local" → "Remote to Local (R2L)" (attack class)
# - "biograph" → "bar graph"
# - "reservoir visualization" → "result visualization"
# - "minest model" → "optimized model"
# - "oxygen regression" → "Logistic Regression"
# - "research cross-parameter" → "Randomized Search Cross-Validation"
# - "confusion metrics" → "confusion matrix"
# - "hit map" → "heatmap"
# - "filter SCY" → "SelectKBest"
# - Added attack class names: DoS, Probe, R2L, U2R
# - Proper ML terminology throughout
