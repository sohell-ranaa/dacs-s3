{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 06: Model Evaluation\n",
    "\n",
    "**What you'll learn:**\n",
    "- Why accuracy alone isn't enough\n",
    "- Confusion matrix (what the model got right/wrong)\n",
    "- Precision, Recall, F1-score\n",
    "- When to use which metric\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: The Problem with Accuracy\n",
    "\n",
    "### READ\n",
    "\n",
    "**Accuracy can be MISLEADING with imbalanced data!**\n",
    "\n",
    "Example: 100 emails, 95 normal, 5 spam\n",
    "- A model that predicts \"normal\" for EVERYTHING gets 95% accuracy!\n",
    "- But it never detects spam - useless as a spam filter.\n",
    "\n",
    "We need better metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRY IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Simulate imbalanced predictions\n",
    "actual = ['normal']*95 + ['spam']*5\n",
    "predicted = ['normal']*100  # Lazy model: always predict normal\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(actual, predicted):.0%}\")\n",
    "print(\"\\nBut this model NEVER detects spam!\")\n",
    "print(\"95% accuracy, 0% spam detection - useless!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Confusion Matrix\n",
    "\n",
    "### READ\n",
    "\n",
    "A **Confusion Matrix** shows exactly what the model got right and wrong.\n",
    "\n",
    "```\n",
    "                    Predicted\n",
    "                 Normal    Attack\n",
    "Actual Normal      TN        FP\n",
    "       Attack      FN        TP\n",
    "```\n",
    "\n",
    "- **TN (True Negative)**: Correctly predicted Normal\n",
    "- **TP (True Positive)**: Correctly predicted Attack\n",
    "- **FP (False Positive)**: Predicted Attack, was Normal (false alarm)\n",
    "- **FN (False Negative)**: Predicted Normal, was Attack (MISSED attack!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRY IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_csv('../datasets/tomatjus.csv')\n",
    "X = df.drop('quality', axis=1)\n",
    "y = df['quality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=model.classes_)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPLAIN\n",
    "\n",
    "Reading the matrix:\n",
    "- **Diagonal** (top-left to bottom-right) = Correct predictions\n",
    "- **Off-diagonal** = Mistakes\n",
    "- Look for which classes are confused with each other\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Precision, Recall, F1-Score\n",
    "\n",
    "### READ\n",
    "\n",
    "**PRECISION**: Of all predicted as Attack, how many really were?\n",
    "```\n",
    "Precision = TP / (TP + FP)\n",
    "\"When I say Attack, am I usually right?\"\n",
    "```\n",
    "\n",
    "**RECALL**: Of all actual Attacks, how many did we catch?\n",
    "```\n",
    "Recall = TP / (TP + FN)\n",
    "\"Am I catching most attacks?\"\n",
    "```\n",
    "\n",
    "**F1-SCORE**: Balance between Precision and Recall\n",
    "```\n",
    "F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRY IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get the full classification report\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPLAIN\n",
    "\n",
    "Reading the report:\n",
    "- **precision**: When model predicts this class, how often is it correct?\n",
    "- **recall**: Of all actual samples of this class, how many did we find?\n",
    "- **f1-score**: Balance of precision and recall\n",
    "- **support**: Number of actual samples in each class\n",
    "\n",
    "**Averages:**\n",
    "- **macro avg**: Simple average (treats all classes equally)\n",
    "- **weighted avg**: Weighted by class size\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Which Metric to Use?\n",
    "\n",
    "| Situation | Use This Metric | Why |\n",
    "|-----------|-----------------|-----|\n",
    "| Balanced data | Accuracy | All classes equally important |\n",
    "| Imbalanced data | F1-score (macro) | Treats all classes equally |\n",
    "| Missing attacks is costly | Recall | Want to catch all attacks |\n",
    "| False alarms are costly | Precision | Want accurate predictions |\n",
    "| General imbalanced | F1-weighted | Considers class sizes |\n",
    "\n",
    "**For your assignment (intrusion detection):**\n",
    "- Missing attacks (FN) is dangerous → Focus on **Recall**\n",
    "- But also need reasonable precision → Use **F1-score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print(\"Individual Metrics:\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, predictions):.3f}\")\n",
    "print(f\"Precision: {precision_score(y_test, predictions, average='weighted'):.3f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, predictions, average='weighted'):.3f}\")\n",
    "print(f\"F1-score:  {f1_score(y_test, predictions, average='weighted'):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# Basic metrics\n",
    "accuracy_score(y_test, predictions)\n",
    "confusion_matrix(y_test, predictions)\n",
    "classification_report(y_test, predictions)\n",
    "\n",
    "# For multi-class, specify average\n",
    "f1_score(y_test, predictions, average='weighted')\n",
    "f1_score(y_test, predictions, average='macro')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Next Lesson\n",
    "\n",
    "In **Lesson 07: Hyperparameter Tuning**, you'll learn:\n",
    "- What are hyperparameters\n",
    "- How to find the best values\n",
    "- GridSearchCV (automated tuning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
